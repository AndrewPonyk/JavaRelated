# =============================================================================
# Concurrent Web Crawler - Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your environment
# DO NOT commit .env files to version control

# -----------------------------------------------------------------------------
# Crawler Configuration
# -----------------------------------------------------------------------------

# Number of worker threads in the thread pool
CRAWLER_THREADS=10

# Maximum concurrent HTTP connections (Semaphore permits)
CRAWLER_MAX_CONNECTIONS=20

# Default delay between requests to same domain (milliseconds)
CRAWLER_DEFAULT_DELAY_MS=1000

# Maximum pages to crawl before stopping
CRAWLER_MAX_PAGES=10000

# Maximum crawl depth from seed URLs
CRAWLER_MAX_DEPTH=10

# Request timeout in milliseconds
CRAWLER_REQUEST_TIMEOUT_MS=30000

# User-Agent string for HTTP requests
CRAWLER_USER_AGENT=ConcurrentCrawler/1.0 (+https://github.com/example/crawler)

# Whether to respect robots.txt (true/false)
CRAWLER_RESPECT_ROBOTS=true

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------

# Path to SQLite database file
DB_PATH=./data/crawler.db

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------

# Log level (TRACE, DEBUG, INFO, WARN, ERROR)
LOG_LEVEL=INFO

# Log file path (optional, logs to console by default)
LOG_FILE=./logs/crawler.log

# -----------------------------------------------------------------------------
# API/Server Configuration
# -----------------------------------------------------------------------------

# Allowed CORS origins for API endpoints (comma-separated, * for all)
ALLOWED_ORIGINS=*

# -----------------------------------------------------------------------------
# Railway-specific (set automatically in Railway environment)
# -----------------------------------------------------------------------------

# PORT=8080
# RAILWAY_ENVIRONMENT=production

# -----------------------------------------------------------------------------
# Optional: Domain Restrictions
# -----------------------------------------------------------------------------

# Comma-separated list of allowed domains (empty = allow all)
# CRAWLER_ALLOWED_DOMAINS=example.com,example.org

# Comma-separated list of blocked domains
# CRAWLER_BLOCKED_DOMAINS=spam.com,ads.example.com

# -----------------------------------------------------------------------------
# Optional: ML/Indexing Configuration
# -----------------------------------------------------------------------------

# Target keywords for relevance scoring (comma-separated)
# CRAWLER_TARGET_KEYWORDS=java,concurrency,web

# Minimum relevance score to save page (0.0 to 1.0)
# CRAWLER_MIN_RELEVANCE=0.1

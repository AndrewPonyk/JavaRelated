Algorithms Quiz - 200 Questions
Tech Stack: Sorting, Searching, Graph, Dynamic Programming, Greedy, Divide-and-Conquer, Backtracking, String, Computational Geometry

1. What is the time complexity of QuickSort in the worst case?

Answer: O(n²) - Worst case occurs when the pivot is always the smallest or largest element, resulting in unbalanced partitions. Average case is O(n log n), and best case is also O(n log n) with good pivot selection.

#@@@@@@@@@@

2. Which of the following are stable sorting algorithms? (Multiple correct)
A) Merge Sort
B) Quick Sort
C) Bubble Sort
D) Insertion Sort
E) Heap Sort
F) Counting Sort

Answer: A, C, D, F - Stable sorting algorithms maintain the relative order of equal elements. Merge Sort, Bubble Sort, Insertion Sort, and Counting Sort are stable. Quick Sort and Heap Sort are not stable in their standard implementations.

#@@@@@@@@@@

3. Complete this binary search implementation:
```python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1

    while left <= right:
        mid = left + (right - left) // 2

        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = _______________
        else:
            right = _______________

    return -1
```

Answer: `mid + 1` and `mid - 1` - Binary search narrows the search space by moving left pointer to mid + 1 when target is greater than middle element, and right pointer to mid - 1 when target is smaller.

#@@@@@@@@@@

4. What is the difference between DFS and BFS graph traversal?

Answer: DFS (Depth-First Search) explores as far as possible along each branch before backtracking, uses stack (recursion), good for topological sorting and cycle detection. BFS (Breadth-First Search) explores all neighbors before moving to next level, uses queue, good for shortest path in unweighted graphs.

#@@@@@@@@@@

5. Find the bug in this dynamic programming solution:
```python
def fibonacci(n, memo={}):
    if n <= 1:
        return n

    if n in memo:
        return memo[n]

    # Bug: mutable default argument
    memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)
    return memo[n]

# Multiple calls
print(fibonacci(10))  # Works correctly
print(fibonacci(5))   # May return incorrect result
```

Answer: Mutable default argument `memo={}` is shared across all function calls, causing incorrect results when calling with smaller values after larger ones. Should use `memo=None` and initialize `memo = {} if memo is None else memo` inside the function.

#@@@@@@@@@@

6. How do you implement Dijkstra's shortest path algorithm?

Answer: Use priority queue (min-heap), initialize distances to infinity except source (0), repeatedly extract minimum distance vertex, update distances to neighbors if shorter path found, continue until all vertices processed. Time complexity: O((V + E) log V) with binary heap.

#@@@@@@@@@@

7. Which of the following are characteristics of greedy algorithms? (Multiple correct)
A) Makes locally optimal choices
B) Guarantees global optimum
C) Uses dynamic programming
D) Cannot be undone once made
E) Works for optimization problems
F) Always polynomial time

Answer: A, D, E - Greedy algorithms make locally optimal choices that cannot be undone, used for optimization problems. They don't guarantee global optimum (only for specific problems), don't use dynamic programming, and time complexity varies by problem.

#@@@@@@@@@@

8. Complete this merge sort implementation:
```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr

    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])

    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0

    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1

    # Add remaining elements
    result.extend(left[_______________])
    result.extend(right[_______________])

    return result
```

Answer: `i:` and `j:` - Extend result with remaining elements from left[i:] and right[j:] after one array is exhausted. This ensures all elements are included in the merged result.

#@@@@@@@@@@

9. What is the purpose of backtracking algorithms?

Answer: Backtracking systematically explores solution space by building solutions incrementally and abandoning candidates ("backtracking") when they cannot lead to valid solutions. Used for constraint satisfaction problems like N-Queens, Sudoku, and generating permutations/combinations.

#@@@@@@@@@@

10. Which of the following problems can be solved using dynamic programming? (Multiple correct)
A) Fibonacci sequence
B) Longest Common Subsequence
C) Knapsack problem
D) Coin change problem
E) Edit distance
F) Matrix chain multiplication

Answer: A, B, C, D, E, F - All are classic dynamic programming problems with overlapping subproblems and optimal substructure. DP provides efficient solutions by storing and reusing previously computed results.

#@@@@@@@@@@

11. Predict the output of this algorithm:
```python
def mystery_algorithm(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return arr

result = mystery_algorithm([64, 34, 25, 12, 22, 11, 90])
print(result)
```

Answer: [11, 12, 22, 25, 34, 64, 90] - This is bubble sort algorithm. It repeatedly steps through the list, compares adjacent elements and swaps them if they're in wrong order, "bubbling" the largest elements to the end.

#@@@@@@@@@@

12. How do you detect cycles in a directed graph?

Answer: Use DFS with three colors: white (unvisited), gray (visiting), black (visited). If during DFS you encounter a gray vertex, there's a back edge indicating a cycle. Alternatively, use topological sorting - if you can't sort all vertices, there's a cycle.

#@@@@@@@@@@

13. Which of the following are divide-and-conquer algorithms? (Multiple correct)
A) Merge Sort
B) Quick Sort
C) Binary Search
D) Strassen's Matrix Multiplication
E) Karatsuba Multiplication
F) Bubble Sort

Answer: A, B, C, D, E - Divide-and-conquer algorithms break problems into smaller subproblems, solve recursively, and combine results. Bubble Sort is not divide-and-conquer; it's a simple comparison-based sorting algorithm.

#@@@@@@@@@@

14. Complete this graph representation:
```python
class Graph:
    def __init__(self, vertices):
        self.vertices = vertices
        self.adj_list = {i: [] for i in range(vertices)}
        self.adj_matrix = [[0] * vertices for _ in range(vertices)]

    def add_edge(self, u, v, weight=1):
        # Adjacency list representation
        self.adj_list[u].append((v, weight))

        # Adjacency matrix representation
        self.adj_matrix[u][v] = _______________

        # For undirected graph, add reverse edge
        # self.adj_list[v].append((u, weight))
        # self.adj_matrix[v][u] = weight
```

Answer: `weight` - Store the weight of the edge in the adjacency matrix. For unweighted graphs, weight would be 1. For weighted graphs, store the actual weight value.

#@@@@@@@@@@

15. What is the time complexity of building a heap from an array?

Answer: O(n) - Building a heap from an unsorted array using the bottom-up approach (heapify) takes linear time. This is more efficient than inserting elements one by one, which would take O(n log n) time.

#@@@@@@@@@@

16. Which of the following string matching algorithms have linear time complexity? (Multiple correct)
A) Naive string matching
B) KMP (Knuth-Morris-Pratt)
C) Rabin-Karp
D) Boyer-Moore
E) Z-algorithm
F) Aho-Corasick

Answer: B, E, F - KMP has O(n+m) time complexity, Z-algorithm is O(n), and Aho-Corasick is O(n+m+z) where z is number of matches. Naive is O(nm), Rabin-Karp is O(nm) worst case, Boyer-Moore is O(nm) worst case but O(n/m) best case.

#@@@@@@@@@@

17. Complete this topological sort implementation:
```python
from collections import deque, defaultdict

def topological_sort(graph, num_vertices):
    # Calculate in-degrees
    in_degree = [0] * num_vertices
    for u in graph:
        for v in graph[u]:
            in_degree[v] += 1

    # Initialize queue with vertices having 0 in-degree
    queue = deque()
    for i in range(num_vertices):
        if in_degree[i] == 0:
            queue.append(i)

    result = []
    while queue:
        vertex = queue.popleft()
        result.append(vertex)

        # Reduce in-degree of adjacent vertices
        for neighbor in graph[vertex]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == _______________:
                queue.append(neighbor)

    return result if len(result) == num_vertices else []
```

Answer: `0` - When a vertex's in-degree becomes 0, it means all its dependencies have been processed, so it can be added to the queue for processing. This is Kahn's algorithm for topological sorting.

#@@@@@@@@@@

18. What is the difference between Prim's and Kruskal's algorithms?

Answer: Both find Minimum Spanning Tree (MST). Prim's grows MST from a starting vertex, uses priority queue, good for dense graphs, O(E log V) with binary heap. Kruskal's sorts all edges, uses Union-Find, good for sparse graphs, O(E log E). Prim's is vertex-based, Kruskal's is edge-based.

#@@@@@@@@@@

19. Find the bug in this quicksort implementation:
```python
def quicksort(arr, low, high):
    if low < high:
        # Partition the array
        pivot_index = partition(arr, low, high)

        # Recursively sort elements before and after partition
        quicksort(arr, low, pivot_index - 1)
        quicksort(arr, pivot_index + 1, high)  # Bug: potential stack overflow

def partition(arr, low, high):
    pivot = arr[high]
    i = low - 1

    for j in range(low, high):
        if arr[j] <= pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]

    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1

# Usage
arr = [64, 34, 25, 12, 22, 11, 90]
quicksort(arr, 0, len(arr) - 1)
```

Answer: No explicit bug in the code logic, but potential stack overflow for worst-case input (already sorted array). The algorithm has O(n²) time complexity and O(n) space complexity in worst case. Should implement iterative version or use randomized pivot selection to avoid worst-case behavior.

#@@@@@@@@@@

20. How do you implement the Longest Common Subsequence (LCS) problem?

Answer: Use dynamic programming with 2D table dp[i][j] representing LCS length of first i characters of string1 and first j characters of string2. If characters match: dp[i][j] = dp[i-1][j-1] + 1, else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]). Time: O(mn), Space: O(mn).

#@@@@@@@@@@

21. Which of the following are properties of red-black trees? (Multiple correct)
A) Every node is either red or black
B) Root is always black
C) Red nodes cannot have red children
D) All paths from root to leaves have same number of black nodes
E) Leaves (NIL nodes) are black
F) Height is O(log n)

Answer: A, B, C, D, E, F - All are properties of red-black trees. These properties ensure the tree remains approximately balanced, guaranteeing O(log n) height and efficient operations.

#@@@@@@@@@@

22. Complete this Floyd-Warshall algorithm:
```python
def floyd_warshall(graph):
    n = len(graph)
    # Initialize distance matrix
    dist = [[float('inf')] * n for _ in range(n)]

    # Copy graph to distance matrix
    for i in range(n):
        for j in range(n):
            if i == j:
                dist[i][j] = 0
            elif graph[i][j] != 0:
                dist[i][j] = graph[i][j]

    # Floyd-Warshall algorithm
    for k in range(n):
        for i in range(n):
            for j in range(n):
                if dist[i][k] + dist[k][j] < dist[i][j]:
                    dist[i][j] = _______________

    return dist
```

Answer: `dist[i][k] + dist[k][j]` - Update the shortest distance from i to j through intermediate vertex k. Floyd-Warshall finds all-pairs shortest paths with O(V³) time complexity.

#@@@@@@@@@@

23. What is the purpose of computational geometry algorithms?

Answer: Computational geometry solves problems involving geometric objects like points, lines, polygons. Applications include computer graphics, robotics, GIS, CAD. Common problems: convex hull, line intersection, point-in-polygon tests, closest pair of points, Voronoi diagrams.

#@@@@@@@@@@

24. Which of the following are NP-complete problems? (Multiple correct)
A) Traveling Salesman Problem
B) Knapsack Problem (0/1)
C) Graph Coloring
D) Hamiltonian Path
E) Subset Sum
F) Sorting

Answer: A, B, C, D, E - All except sorting are NP-complete problems. These problems have no known polynomial-time algorithms and are believed to require exponential time. Sorting can be done in O(n log n) time.

#@@@@@@@@@@

25. Find the issue in this binary tree traversal:
```python
class TreeNode:
    def __init__(self, val=0, left=None, right=None):
        self.val = val
        self.left = left
        self.right = right

def inorder_traversal(root):
    result = []

    def inorder(node):
        if node:
            inorder(node.left)
            result.append(node.val)
            inorder(node.right)

    inorder(root)
    return result

def preorder_traversal(root):
    if not root:
        return []

    stack = [root]
    result = []

    while stack:
        node = stack.pop()
        result.append(node.val)

        # Push right first, then left (so left is processed first)
        if node.right:
            stack.append(node.right)
        if node.left:
            stack.append(node.left)

    return result
```

Answer: No explicit bug in the code. Both implementations are correct - inorder uses recursion and preorder uses iterative approach with stack. The iterative version correctly pushes right child first so left child is processed first (LIFO stack behavior).

#@@@@@@@@@@

26. How do you implement the Knapsack problem using dynamic programming?

Answer: Use 2D DP table dp[i][w] representing maximum value using first i items with weight limit w. Recurrence: dp[i][w] = max(dp[i-1][w], dp[i-1][w-weight[i]] + value[i]) if weight[i] <= w, else dp[i-1][w]. Time: O(nW), Space: O(nW), can be optimized to O(W).

#@@@@@@@@@@

27. Which of the following are characteristics of AVL trees? (Multiple correct)
A) Self-balancing binary search tree
B) Height difference between subtrees ≤ 1
C) Rotations maintain balance
D) Worst-case height is O(log n)
E) More balanced than red-black trees
F) Faster insertions than red-black trees

Answer: A, B, C, D, E - AVL trees are self-balancing BSTs with height difference ≤ 1, use rotations, guarantee O(log n) height, and are more strictly balanced than red-black trees. However, red-black trees have faster insertions due to fewer rotations required.

#@@@@@@@@@@

28. Complete this Bellman-Ford algorithm:
```python
def bellman_ford(graph, source):
    # Initialize distances
    distances = {vertex: float('inf') for vertex in graph}
    distances[source] = 0

    # Relax edges V-1 times
    for _ in range(len(graph) - 1):
        for u in graph:
            for v, weight in graph[u]:
                if distances[u] + weight < distances[v]:
                    distances[v] = _______________

    # Check for negative cycles
    for u in graph:
        for v, weight in graph[u]:
            if distances[u] + weight < distances[v]:
                return None  # Negative cycle detected

    return distances
```

Answer: `distances[u] + weight` - Update the distance to vertex v if a shorter path is found through vertex u. Bellman-Ford can handle negative weights and detects negative cycles, unlike Dijkstra's algorithm.

#@@@@@@@@@@

29. What is the difference between stable and unstable sorting algorithms?

Answer: Stable sorting maintains the relative order of equal elements, while unstable sorting may change their relative positions. Stable: Merge Sort, Insertion Sort, Bubble Sort, Counting Sort. Unstable: Quick Sort, Heap Sort, Selection Sort. Stability is important when sorting objects with multiple keys.

#@@@@@@@@@@

30. Which of the following problems use the sliding window technique? (Multiple correct)
A) Maximum sum subarray of size k
B) Longest substring without repeating characters
C) Minimum window substring
D) Find all anagrams in a string
E) Longest common subsequence
F) Maximum product subarray

Answer: A, B, C, D - Sliding window technique is used for problems involving contiguous subarrays/substrings. LCS uses dynamic programming, and maximum product subarray uses modified Kadane's algorithm, not sliding window.

#@@@@@@@@@@

31. Predict the output of this recursive algorithm:
```python
def mystery_function(n):
    if n <= 1:
        return n
    return mystery_function(n-1) + mystery_function(n-2)

def optimized_version(n, memo={}):
    if n <= 1:
        return n
    if n in memo:
        return memo[n]
    memo[n] = optimized_version(n-1, memo) + optimized_version(n-2, memo)
    return memo[n]

print(mystery_function(5))
print(optimized_version(5))
```

Answer: Both print 5. This is the Fibonacci sequence: F(5) = F(4) + F(3) = 3 + 2 = 5. The first version has O(2^n) time complexity, the optimized version with memoization has O(n) time complexity.

#@@@@@@@@@@

32. How do you implement Union-Find (Disjoint Set) data structure?

Answer: Use array to represent parent pointers, implement find() with path compression, union() with union by rank/size. Path compression flattens tree during find operations. Union by rank attaches smaller tree under larger tree. Both optimizations give nearly O(1) amortized time complexity.

#@@@@@@@@@@

33. Which of the following are applications of graph algorithms? (Multiple correct)
A) Social network analysis
B) GPS navigation systems
C) Compiler optimization
D) Network routing protocols
E) Recommendation systems
F) Image processing

Answer: A, B, C, D, E, F - All are applications of graph algorithms. Social networks (connectivity), GPS (shortest paths), compilers (dependency graphs), routing (network topology), recommendations (similarity graphs), image processing (pixel connectivity).

#@@@@@@@@@@

34. Find the bug in this binary search tree implementation:
```python
class BST:
    def __init__(self):
        self.root = None

    def insert(self, val):
        self.root = self._insert(self.root, val)

    def _insert(self, node, val):
        if not node:
            return TreeNode(val)

        if val < node.val:
            node.left = self._insert(node.left, val)
        elif val > node.val:
            node.right = self._insert(node.right, val)
        # Bug: doesn't handle duplicate values

        return node

    def search(self, val):
        return self._search(self.root, val)

    def _search(self, node, val):
        if not node or node.val == val:
            return node

        if val < node.val:
            return self._search(node.left, val)
        return self._search(node.right, val)
```

Answer: The code doesn't handle duplicate values - they are silently ignored. Depending on requirements, should either: 1) Allow duplicates by using <= instead of <, 2) Update existing node, 3) Explicitly handle duplicates, or 4) Document that duplicates are not allowed.

#@@@@@@@@@@

35. What is the time complexity of the Euclidean algorithm for GCD?

Answer: O(log(min(a,b))) - The Euclidean algorithm for finding Greatest Common Divisor has logarithmic time complexity. Each step reduces the problem size by at least half, leading to efficient computation even for large numbers.

#@@@@@@@@@@

36. Which of the following are valid approaches for solving the Two Sum problem? (Multiple correct)
A) Brute force O(n²)
B) Hash table O(n)
C) Two pointers O(n log n)
D) Binary search O(n log n)
E) Sorting + binary search O(n log n)
F) Dynamic programming O(n²)

Answer: A, B, C, D, E - All except DP are valid approaches. Brute force checks all pairs, hash table stores complements, two pointers works on sorted array, binary search finds complement for each element. DP is not applicable to Two Sum problem.

#@@@@@@@@@@

37. Complete this trie (prefix tree) implementation:
```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end_of_word = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.is_end_of_word = True

    def search(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return _______________

    def starts_with(self, prefix):
        node = self.root
        for char in prefix:
            if char not in node.children:
                return False
            node = node.children[char]
        return True
```

Answer: `node.is_end_of_word` - Returns True only if the word exists as a complete word in the trie, not just as a prefix. This distinguishes between searching for complete words vs. checking if a prefix exists.

#@@@@@@@@@@

38. What is the difference between Kadane's algorithm and brute force for maximum subarray?

Answer: Kadane's algorithm finds maximum subarray sum in O(n) time using dynamic programming principle - at each position, decide whether to extend previous subarray or start new one. Brute force checks all possible subarrays in O(n²) or O(n³) time. Kadane's is optimal.

#@@@@@@@@@@

39. Find the bug in this graph cycle detection:
```python
def has_cycle_undirected(graph):
    visited = set()

    def dfs(node, parent):
        visited.add(node)

        for neighbor in graph[node]:
            if neighbor not in visited:
                if dfs(neighbor, node):
                    return True
            elif neighbor != parent:  # Back edge found
                return True

        return False

    # Check all components
    for node in graph:
        if node not in visited:
            if dfs(node, None):  # Bug: should pass -1 or use different approach
                return True

    return False

# Test with disconnected graph
graph = {
    0: [1],
    1: [0, 2],
    2: [1],
    3: [4],
    4: [3]
}
```

Answer: No explicit bug in the logic. The code correctly detects cycles in undirected graphs. Passing None as parent for the starting node is acceptable since we check `neighbor != parent`. The algorithm correctly handles disconnected components.

#@@@@@@@@@@

40. How do you implement the Sieve of Eratosthenes?

Answer: Create boolean array of size n+1, mark 0 and 1 as non-prime, for each prime p starting from 2, mark all multiples of p as non-prime. Optimization: start marking from p², iterate only up to √n. Time complexity: O(n log log n), space: O(n).

#@@@@@@@@@@

41. Which of the following are characteristics of B-trees? (Multiple correct)
A) Self-balancing tree
B) Nodes can have multiple keys
C) All leaves at same level
D) Used in databases and file systems
E) Minimum degree t ≥ 2
F) Internal nodes have t to 2t children

Answer: A, B, C, D, E, F - All are B-tree characteristics. B-trees are self-balancing with multiple keys per node, all leaves at same level, widely used in databases, have minimum degree t, and internal nodes have between t and 2t children (except root).

#@@@@@@@@@@

42. Complete this edit distance (Levenshtein distance) algorithm:
```python
def edit_distance(str1, str2):
    m, n = len(str1), len(str2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    # Initialize base cases
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    # Fill the DP table
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if str1[i-1] == str2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(
                    dp[i-1][j],    # deletion
                    dp[i][j-1],    # insertion
                    _______________  # substitution
                )

    return dp[m][n]
```

Answer: `dp[i-1][j-1]` - Substitution operation replaces character at position i-1 in str1 with character at position j-1 in str2, so we take the cost from dp[i-1][j-1] and add 1 for the substitution.

#@@@@@@@@@@

43. What is the purpose of segment trees?

Answer: Segment trees efficiently answer range queries (sum, min, max) and support updates on arrays. Build: O(n), Query: O(log n), Update: O(log n). Each node represents a range, leaves represent individual elements. Useful for problems requiring frequent range operations.

#@@@@@@@@@@

44. Which of the following are properties of hash tables? (Multiple correct)
A) Average O(1) insertion and lookup
B) Worst-case O(n) operations
C) Require good hash function
D) May need resizing
E) Support ordered iteration
F) Handle collisions

Answer: A, B, C, D, F - Hash tables provide average O(1) operations but O(n) worst-case, need good hash functions, may require resizing, and must handle collisions. They don't support ordered iteration (unlike balanced trees).

#@@@@@@@@@@

45. Find the issue in this matrix multiplication algorithm:
```python
def matrix_multiply(A, B):
    rows_A, cols_A = len(A), len(A[0])
    rows_B, cols_B = len(B), len(B[0])

    # Check if multiplication is possible
    if cols_A != rows_B:
        raise ValueError("Cannot multiply matrices")

    # Initialize result matrix
    C = [[0] * cols_B for _ in range(rows_A)]

    # Multiply matrices
    for i in range(rows_A):
        for j in range(cols_B):
            for k in range(cols_A):
                C[i][j] += A[i][k] * B[k][j]

    return C

# Test with large matrices - performance issue
A = [[1] * 1000 for _ in range(1000)]
B = [[1] * 1000 for _ in range(1000)]
result = matrix_multiply(A, B)  # O(n³) - very slow for large matrices
```

Answer: No bug in correctness, but performance issue. Standard matrix multiplication is O(n³). For large matrices, should consider: 1) Strassen's algorithm O(n^2.807), 2) Parallel computation, 3) Optimized libraries (NumPy), 4) Block matrix multiplication for cache efficiency.

#@@@@@@@@@@

46. How do you implement the Longest Increasing Subsequence (LIS) problem?

Answer: DP approach: dp[i] = length of LIS ending at index i. For each element, check all previous elements and extend the longest valid subsequence. Time: O(n²). Optimized: Use binary search with patience sorting approach for O(n log n) time complexity.

#@@@@@@@@@@

47. Which of the following are valid tree traversal orders? (Multiple correct)
A) Preorder (Root, Left, Right)
B) Inorder (Left, Root, Right)
C) Postorder (Left, Right, Root)
D) Level-order (Breadth-first)
E) Reverse inorder (Right, Root, Left)
F) Morris traversal

Answer: A, B, C, D, E, F - All are valid tree traversal methods. Preorder, inorder, and postorder are depth-first traversals. Level-order uses BFS. Reverse inorder traverses right subtree first. Morris traversal achieves O(1) space complexity using threading.

#@@@@@@@@@@

48. Complete this convex hull algorithm (Graham scan):
```python
import math

def orientation(p, q, r):
    val = (q[1] - p[1]) * (r[0] - q[0]) - (q[0] - p[0]) * (r[1] - q[1])
    if val == 0:
        return 0  # collinear
    return 1 if val > 0 else 2  # clockwise or counterclockwise

def convex_hull(points):
    n = len(points)
    if n < 3:
        return []

    # Find bottom-most point (or leftmost in case of tie)
    l = 0
    for i in range(1, n):
        if points[i][1] < points[l][1]:
            l = i
        elif points[i][1] == points[l][1] and points[i][0] < points[l][0]:
            l = i

    # Start from leftmost point, keep moving counterclockwise
    hull = []
    p = l
    while True:
        hull.append(points[p])

        # Find the most counterclockwise point
        q = (p + 1) % n
        for i in range(n):
            if orientation(points[p], points[i], points[q]) == _______________:
                q = i

        p = q
        if p == l:  # Came back to start point
            break

    return hull
```

Answer: `2` - We want counterclockwise orientation (value 2) to find the convex hull. This is the Jarvis march (Gift wrapping) algorithm. For each point, we find the most counterclockwise point to continue building the hull.

#@@@@@@@@@@

49. What is the difference between memoization and tabulation in dynamic programming?

Answer: Memoization is top-down approach using recursion with caching (usually hash table), solves subproblems on demand. Tabulation is bottom-up approach filling table iteratively, solves all subproblems. Memoization may have function call overhead, tabulation is more space-efficient and iterative.

#@@@@@@@@@@

50. Which of the following problems use the two-pointer technique? (Multiple correct)
A) Two Sum (sorted array)
B) Three Sum
C) Container with most water
D) Remove duplicates from sorted array
E) Merge two sorted arrays
F) Palindrome checking

Answer: A, B, C, D, E, F - All use two-pointer technique. Two Sum uses left/right pointers, Three Sum fixes one element and uses two pointers, Container uses left/right for maximum area, Remove duplicates uses slow/fast pointers, Merge uses pointers for each array, Palindrome uses start/end pointers.

#@@@@@@@@@@

51. Predict the output of this recursive algorithm:
```python
def tower_of_hanoi(n, source, destination, auxiliary):
    if n == 1:
        print(f"Move disk 1 from {source} to {destination}")
        return

    tower_of_hanoi(n-1, source, auxiliary, destination)
    print(f"Move disk {n} from {source} to {destination}")
    tower_of_hanoi(n-1, auxiliary, destination, source)

tower_of_hanoi(3, 'A', 'C', 'B')
```

Answer: Outputs 7 moves to solve Tower of Hanoi with 3 disks:
Move disk 1 from A to C
Move disk 2 from A to B
Move disk 1 from C to B
Move disk 3 from A to C
Move disk 1 from B to A
Move disk 2 from B to C
Move disk 1 from A to C
Time complexity: O(2^n - 1).

#@@@@@@@@@@

52. How do you implement a priority queue using a binary heap?

Answer: Use array representation where parent of index i is at (i-1)//2, children at 2*i+1 and 2*i+2. Insert: add at end, bubble up. Extract: replace root with last element, bubble down. Both operations: O(log n). Heapify array: O(n).

#@@@@@@@@@@

53. Which of the following are characteristics of skip lists? (Multiple correct)
A) Probabilistic data structure
B) Multiple levels of linked lists
C) Expected O(log n) search time
D) Randomized insertion
E) Alternative to balanced trees
F) Deterministic height

Answer: A, B, C, D, E - Skip lists are probabilistic structures with multiple levels, expected O(log n) operations, use randomization for level assignment, and serve as alternatives to balanced trees. Height is probabilistic, not deterministic.

#@@@@@@@@@@

54. Find the bug in this LRU cache implementation:
```python
class LRUCache:
    def __init__(self, capacity):
        self.capacity = capacity
        self.cache = {}
        self.order = []  # Bug: using list for order tracking

    def get(self, key):
        if key in self.cache:
            # Move to end (most recently used)
            self.order.remove(key)  # O(n) operation
            self.order.append(key)
            return self.cache[key]
        return -1

    def put(self, key, value):
        if key in self.cache:
            self.order.remove(key)  # O(n) operation
            self.order.append(key)
            self.cache[key] = value
        else:
            if len(self.cache) >= self.capacity:
                # Remove least recently used
                lru_key = self.order.pop(0)  # O(n) operation
                del self.cache[lru_key]

            self.cache[key] = value
            self.order.append(key)
```

Answer: Using list for order tracking makes operations O(n) instead of O(1). Should use doubly linked list with hash map pointing to nodes, or use OrderedDict in Python which provides O(1) operations for moving elements to end.

#@@@@@@@@@@

55. What is the time complexity of the Fast Fourier Transform (FFT)?

Answer: O(n log n) - FFT reduces the time complexity of Discrete Fourier Transform from O(n²) to O(n log n) using divide-and-conquer approach. It's widely used in signal processing, polynomial multiplication, and convolution operations.

#@@@@@@@@@@

56. Which of the following are valid approaches for finding the median of two sorted arrays? (Multiple correct)
A) Merge arrays and find middle element O(m+n)
B) Binary search on smaller array O(log(min(m,n)))
C) Use two pointers O(m+n)
D) Divide and conquer O(log(m+n))
E) Priority queue approach O(m+n)
F) Linear scan O(m+n)

Answer: A, B, C, E, F - All except D are valid approaches. Binary search on smaller array is optimal O(log(min(m,n))). Merge, two pointers, priority queue, and linear scan all work in O(m+n). Pure divide-and-conquer doesn't directly apply to this problem.

#@@@@@@@@@@

57. Complete this suffix array construction:
```python
def build_suffix_array(text):
    n = len(text)
    suffixes = []

    # Create all suffixes with their starting indices
    for i in range(n):
        suffixes.append((text[i:], i))

    # Sort suffixes lexicographically
    suffixes.sort(key=lambda x: x[0])

    # Extract the starting indices
    suffix_array = [suffix[1] for suffix in suffixes]
    return suffix_array

def lcp_array(text, suffix_array):
    n = len(text)
    rank = [0] * n
    lcp = [0] * (n - 1)

    # Build rank array
    for i in range(n):
        rank[suffix_array[i]] = i

    # Compute LCP array
    h = 0
    for i in range(n):
        if rank[i] > 0:
            j = suffix_array[rank[i] - 1]
            while i + h < n and j + h < n and text[i + h] == text[j + h]:
                h += 1
            lcp[rank[i] - 1] = h
            if h > 0:
                h -= _______________

    return lcp
```

Answer: `1` - In the LCP array construction using Kasai's algorithm, we decrement h by 1 because when moving from suffix i to suffix i+1, the LCP with the previous suffix in sorted order can decrease by at most 1. This optimization gives O(n) time complexity.

#@@@@@@@@@@

58. What is the difference between Breadth-First Search and Dijkstra's algorithm?

Answer: BFS finds shortest path in unweighted graphs using queue, O(V+E) time. Dijkstra finds shortest path in weighted graphs (non-negative weights) using priority queue, O((V+E)log V) time. BFS is special case of Dijkstra when all edge weights are 1.

#@@@@@@@@@@

59. Find the bug in this coin change dynamic programming solution:
```python
def coin_change(coins, amount):
    # Initialize DP array
    dp = [float('inf')] * (amount + 1)
    dp[0] = 0

    # Fill DP array
    for i in range(1, amount + 1):
        for coin in coins:
            if coin <= i:
                dp[i] = min(dp[i], dp[i - coin] + 1)

    return dp[amount] if dp[amount] != float('inf') else -1

# Test cases
print(coin_change([1, 3, 4], 6))  # Should return 2 (3+3)
print(coin_change([2], 3))        # Should return -1 (impossible)
print(coin_change([1], 0))        # Should return 0
```

Answer: No bug in the implementation. The algorithm correctly solves the coin change problem using dynamic programming. It finds the minimum number of coins needed to make the given amount, returns -1 if impossible, and handles edge cases properly.

#@@@@@@@@@@

60. How do you implement the Manacher's algorithm for palindrome detection?

Answer: Manacher's algorithm finds all palindromes in O(n) time. Transform string by inserting special characters, maintain center and right boundary of rightmost palindrome, use symmetry property to avoid redundant comparisons. For each position, use previously computed information to expand around center efficiently.

#@@@@@@@@@@

61. Which of the following are properties of Fenwick Trees (Binary Indexed Trees)? (Multiple correct)
A) Support range sum queries
B) Support point updates
C) O(log n) query and update time
D) Use O(n) space
E) Based on binary representation
F) Support range updates

Answer: A, B, C, D, E - Fenwick trees support range sum queries and point updates in O(log n) time using O(n) space, based on binary representation of indices. Standard Fenwick trees don't support range updates (need difference array technique for that).

#@@@@@@@@@@

62. Complete this A* pathfinding algorithm:
```python
import heapq

def a_star(graph, start, goal, heuristic):
    open_set = [(0, start)]
    came_from = {}
    g_score = {start: 0}
    f_score = {start: heuristic(start, goal)}

    while open_set:
        current_f, current = heapq.heappop(open_set)

        if current == goal:
            # Reconstruct path
            path = []
            while current in came_from:
                path.append(current)
                current = came_from[current]
            path.append(start)
            return path[::-1]

        for neighbor, weight in graph[current]:
            tentative_g = g_score[current] + weight

            if neighbor not in g_score or tentative_g < g_score[neighbor]:
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g
                f_score[neighbor] = tentative_g + _______________
                heapq.heappush(open_set, (f_score[neighbor], neighbor))

    return None  # No path found
```

Answer: `heuristic(neighbor, goal)` - A* uses f(n) = g(n) + h(n) where g(n) is actual cost from start and h(n) is heuristic estimate to goal. The heuristic must be admissible (never overestimate) for optimal results.

#@@@@@@@@@@

63. What is the purpose of bloom filters?

Answer: Bloom filters are space-efficient probabilistic data structures for membership testing. They can have false positives but never false negatives. Use multiple hash functions and bit array. Applications: web caching, database query optimization, distributed systems for reducing network calls.

#@@@@@@@@@@

64. Which of the following are characteristics of radix sort? (Multiple correct)
A) Non-comparison based sorting
B) Stable sorting algorithm
C) O(d * (n + k)) time complexity
D) Works with integers and strings
E) In-place sorting
F) Linear time for fixed-width keys

Answer: A, B, C, D, F - Radix sort is non-comparison based, stable, has O(d*(n+k)) complexity where d is digits and k is range, works with integers/strings, and is linear for fixed-width keys. It's not in-place as it requires additional space for counting sort.

#@@@@@@@@@@

65. Find the issue in this graph coloring algorithm:
```python
def graph_coloring(graph, num_colors):
    n = len(graph)
    colors = [-1] * n

    def is_safe(vertex, color):
        for neighbor in graph[vertex]:
            if colors[neighbor] == color:
                return False
        return True

    def solve(vertex):
        if vertex == n:
            return True

        for color in range(num_colors):
            if is_safe(vertex, color):
                colors[vertex] = color

                if solve(vertex + 1):
                    return True

                colors[vertex] = -1  # backtrack

        return False

    if solve(0):
        return colors
    return None

# Test with graph that needs more colors than provided
graph = [[1, 2, 3], [0, 2], [0, 1, 3], [0, 2]]  # K4 complete graph
result = graph_coloring(graph, 2)  # Needs at least 4 colors, but only 2 provided
print(result)  # Returns None - correct behavior
```

Answer: No bug in the implementation. The algorithm correctly implements backtracking for graph coloring. It returns None when the graph cannot be colored with the given number of colors, which is the expected behavior for a K4 complete graph with only 2 colors.

#@@@@@@@@@@

66. How do you implement the Boyer-Moore string matching algorithm?

Answer: Boyer-Moore uses two heuristics: bad character rule (skip based on mismatched character) and good suffix rule (skip based on matched suffix). Preprocess pattern to build bad character table and good suffix table. Search from right to left in pattern, use maximum skip from both rules.

#@@@@@@@@@@

67. Which of the following problems can be solved using bit manipulation? (Multiple correct)
A) Finding single number in array of duplicates
B) Counting set bits
C) Power of 2 check
D) Subset generation
E) XOR operations
F) Fast multiplication/division by powers of 2

Answer: A, B, C, D, E, F - All can be solved using bit manipulation. Single number (XOR), counting bits (Brian Kernighan's algorithm), power of 2 (n & (n-1) == 0), subsets (bitmask), XOR operations (direct), fast multiplication/division (left/right shift).

#@@@@@@@@@@

68. Complete this interval scheduling algorithm:
```python
def interval_scheduling(intervals):
    # Sort intervals by end time
    intervals.sort(key=lambda x: x[1])

    selected = []
    last_end_time = float('-inf')

    for start, end in intervals:
        if start >= last_end_time:
            selected.append((start, end))
            last_end_time = _______________

    return selected

# Test case
intervals = [(1, 3), (2, 4), (3, 5), (4, 6)]
result = interval_scheduling(intervals)
print(result)  # Should select non-overlapping intervals
```

Answer: `end` - Update last_end_time to the end time of the currently selected interval. This greedy algorithm selects the maximum number of non-overlapping intervals by always choosing the interval that ends earliest.

#@@@@@@@@@@

69. What is the difference between stable and unstable matching algorithms?

Answer: Stable matching (like Gale-Shapley algorithm) ensures no two participants prefer each other over their current matches. Used in medical residency matching, marriage problems. Unstable matching may have blocking pairs. Stable matching guarantees no incentive to deviate from the matching.

#@@@@@@@@@@

70. Which of the following are applications of minimum spanning trees? (Multiple correct)
A) Network design (connecting cities with minimum cost)
B) Clustering algorithms
C) Image segmentation
D) Approximation algorithms for TSP
E) Circuit design
F) Social network analysis

Answer: A, B, C, D, E, F - All are MST applications. Network design (minimize connection costs), clustering (single-linkage), image segmentation (pixel connectivity), TSP approximation (MST-based heuristics), circuit design (minimize wire length), social networks (community detection).

#@@@@@@@@@@

71. Predict the output of this backtracking algorithm:
```python
def generate_parentheses(n):
    result = []

    def backtrack(current, open_count, close_count):
        if len(current) == 2 * n:
            result.append(current)
            return

        if open_count < n:
            backtrack(current + '(', open_count + 1, close_count)

        if close_count < open_count:
            backtrack(current + ')', open_count, close_count + 1)

    backtrack('', 0, 0)
    return result

print(generate_parentheses(3))
```

Answer: ['((()))', '(()())', '(())()', '()(())', '()()()'] - Generates all valid combinations of 3 pairs of parentheses. The algorithm ensures we never have more closing brackets than opening brackets at any point.

#@@@@@@@@@@

72. How do you implement the Rabin-Karp string matching algorithm?

Answer: Use rolling hash to compute hash values efficiently. Calculate hash of pattern and first window of text. Slide window through text, update hash in O(1) using rolling hash formula. When hashes match, verify character by character to avoid false positives from hash collisions.

#@@@@@@@@@@

73. Which of the following are properties of perfect hashing? (Multiple correct)
A) No collisions
B) O(1) worst-case lookup time
C) Two-level hashing scheme
D) Requires knowledge of all keys in advance
E) Uses universal hash functions
F) Space complexity O(n)

Answer: A, B, C, D, E, F - Perfect hashing eliminates collisions, guarantees O(1) worst-case lookup, uses two-level scheme (first level reduces collisions, second level eliminates them), requires static key set, uses universal hashing, and achieves O(n) space complexity.

#@@@@@@@@@@

74. Find the bug in this longest palindromic substring algorithm:
```python
def longest_palindromic_substring(s):
    if not s:
        return ""

    start = 0
    max_len = 1

    def expand_around_center(left, right):
        while left >= 0 and right < len(s) and s[left] == s[right]:
            left -= 1
            right += 1
        return right - left - 1  # Length of palindrome

    for i in range(len(s)):
        # Check for odd-length palindromes
        len1 = expand_around_center(i, i)
        # Check for even-length palindromes
        len2 = expand_around_center(i, i + 1)

        current_max = max(len1, len2)
        if current_max > max_len:
            max_len = current_max
            start = i - (current_max - 1) // 2

    return s[start:start + max_len]

# Test cases
print(longest_palindromic_substring("babad"))  # "bab" or "aba"
print(longest_palindromic_substring("cbbd"))   # "bb"
```

Answer: No bug in the implementation. The algorithm correctly finds the longest palindromic substring using the expand around centers approach. It handles both odd and even length palindromes and correctly calculates the starting position.

#@@@@@@@@@@

75. What is the time complexity of matrix exponentiation for computing Fibonacci numbers?

Answer: O(log n) - Matrix exponentiation uses the recurrence relation in matrix form and applies fast exponentiation. The 2x2 matrix [[1,1],[1,0]] raised to power n gives Fibonacci numbers. This is much faster than the naive O(n) iterative approach for large n.

#@@@@@@@@@@

76. Which of the following are valid approaches for the Traveling Salesman Problem? (Multiple correct)
A) Brute force O(n!)
B) Dynamic programming with bitmasks O(n²2ⁿ)
C) Nearest neighbor heuristic
D) Christofides algorithm (approximation)
E) Genetic algorithm
F) Branch and bound

Answer: A, B, C, D, E, F - All are valid TSP approaches. Brute force checks all permutations, DP with bitmasks (Held-Karp), nearest neighbor is simple heuristic, Christofides gives 1.5-approximation, genetic algorithms are metaheuristics, branch and bound prunes search space.

#@@@@@@@@@@

77. Complete this Mo's algorithm for range queries:
```python
import math

def mo_algorithm(queries, arr):
    n = len(arr)
    block_size = int(math.sqrt(n))

    # Sort queries by Mo's order
    queries.sort(key=lambda x: (x[0] // block_size, x[1]))

    # Initialize variables
    current_left = 0
    current_right = -1
    current_answer = 0
    answers = [0] * len(queries)

    def add(index):
        nonlocal current_answer
        # Add arr[index] to current window
        current_answer += arr[index]  # Example: sum query

    def remove(index):
        nonlocal current_answer
        # Remove arr[index] from current window
        current_answer -= arr[index]

    for i, (left, right) in enumerate(queries):
        # Expand or shrink the window
        while current_right < right:
            current_right += 1
            add(current_right)

        while current_right > right:
            remove(current_right)
            current_right -= 1

        while current_left < left:
            remove(current_left)
            current_left += 1

        while current_left > left:
            current_left -= 1
            _______________

        answers[i] = current_answer

    return answers
```

Answer: `add(current_left)` - When expanding the window to the left, we need to add the new element at current_left to our current answer. Mo's algorithm efficiently processes range queries by reordering them to minimize pointer movements.

#@@@@@@@@@@

78. What is the difference between online and offline algorithms?

Answer: Online algorithms process input sequentially without knowing future inputs, must make decisions immediately. Offline algorithms have access to entire input before processing, can optimize globally. Online algorithms are measured by competitive ratio against optimal offline solution.

#@@@@@@@@@@

79. Find the bug in this maximum flow algorithm (Ford-Fulkerson):
```python
from collections import defaultdict, deque

class MaxFlow:
    def __init__(self, vertices):
        self.vertices = vertices
        self.graph = defaultdict(lambda: defaultdict(int))

    def add_edge(self, u, v, capacity):
        self.graph[u][v] = capacity

    def bfs(self, source, sink, parent):
        visited = set([source])
        queue = deque([source])

        while queue:
            u = queue.popleft()

            for v in self.graph[u]:
                if v not in visited and self.graph[u][v] > 0:
                    visited.add(v)
                    parent[v] = u
                    if v == sink:
                        return True
                    queue.append(v)
        return False

    def ford_fulkerson(self, source, sink):
        parent = {}
        max_flow = 0

        while self.bfs(source, sink, parent):
            # Find minimum residual capacity along the path
            path_flow = float('inf')
            s = sink
            while s != source:
                path_flow = min(path_flow, self.graph[parent[s]][s])
                s = parent[s]

            # Add path flow to overall flow
            max_flow += path_flow

            # Update residual capacities
            v = sink
            while v != source:
                u = parent[v]
                self.graph[u][v] -= path_flow
                self.graph[v][u] += path_flow  # Add reverse edge
                v = parent[v]

        return max_flow
```

Answer: No explicit bug in the implementation. The Ford-Fulkerson algorithm is correctly implemented with BFS (making it Edmonds-Karp). It properly handles residual graph, reverse edges, and finds maximum flow. The algorithm terminates when no augmenting path exists.

#@@@@@@@@@@

80. How do you implement the Z-algorithm for string matching?

Answer: Z-algorithm computes Z-array where Z[i] is length of longest substring starting from i which is also prefix of string. Use Z-box optimization: maintain leftmost and rightmost indices of Z-box, use previously computed values to avoid redundant comparisons. Time: O(n), Space: O(n).

#@@@@@@@@@@

81. Which of the following are characteristics of randomized algorithms? (Multiple correct)
A) Use random choices during execution
B) May give different outputs on same input
C) Expected time complexity analysis
D) Monte Carlo vs Las Vegas types
E) Can be faster than deterministic algorithms
F) Always give correct results

Answer: A, B, C, D, E - Randomized algorithms use random choices, may vary in output/runtime, analyzed by expected complexity, classified as Monte Carlo (may err) or Las Vegas (always correct but random runtime), can outperform deterministic algorithms. They don't always give correct results (Monte Carlo type).

#@@@@@@@@@@

82. Complete this suffix tree construction (Ukkonen's algorithm):
```python
class SuffixTreeNode:
    def __init__(self):
        self.children = {}
        self.suffix_link = None
        self.start = -1
        self.end = None
        self.suffix_index = -1

class SuffixTree:
    def __init__(self, text):
        self.text = text
        self.root = SuffixTreeNode()
        self.active_node = self.root
        self.active_edge = -1
        self.active_length = 0
        self.remaining_suffix_count = 0
        self.leaf_end = -1
        self.root_end = None
        self.split_end = None
        self.size = len(text)

        for i in range(self.size):
            self.extend_suffix_tree(i)

    def extend_suffix_tree(self, pos):
        self.leaf_end = pos
        self.remaining_suffix_count += 1
        last_new_node = None

        while self.remaining_suffix_count > 0:
            if self.active_length == 0:
                self.active_edge = pos

            if self.text[self.active_edge] not in self.active_node.children:
                # Create new leaf edge
                self.active_node.children[self.text[self.active_edge]] = SuffixTreeNode()
                self.active_node.children[self.text[self.active_edge]].start = pos

                if last_new_node is not None:
                    last_new_node.suffix_link = self.active_node
                    last_new_node = None
            else:
                # Walk down the tree
                next_node = self.active_node.children[self.text[self.active_edge]]
                if self.walk_down(next_node):
                    continue

                # Check if current character already exists
                if self.text[next_node.start + self.active_length] == self.text[pos]:
                    if last_new_node is not None and self.active_node != self.root:
                        last_new_node.suffix_link = self.active_node
                        last_new_node = None

                    self.active_length += 1
                    break

                # Split the edge
                self.split_end = next_node.start + self.active_length - 1
                split_node = SuffixTreeNode()
                split_node.start = next_node.start
                split_node.end = self.split_end

                self.active_node.children[self.text[self.active_edge]] = split_node

                # Create new leaf
                split_node.children[self.text[pos]] = SuffixTreeNode()
                split_node.children[self.text[pos]].start = pos

                next_node.start += self.active_length
                split_node.children[self.text[next_node.start]] = next_node

                if last_new_node is not None:
                    last_new_node.suffix_link = split_node

                last_new_node = split_node

            self.remaining_suffix_count -= 1

            if self.active_node == self.root and self.active_length > 0:
                self.active_length -= 1
                self.active_edge = pos - self.remaining_suffix_count + 1
            elif self.active_node != self.root:
                self.active_node = self.active_node.suffix_link or self.root

    def walk_down(self, node):
        edge_length = self.edge_length(node)
        if self.active_length >= edge_length:
            self.active_edge += edge_length
            self.active_length -= edge_length
            self.active_node = node
            return True
        return False

    def edge_length(self, node):
        if node.end is None:
            return self.leaf_end - node.start + 1
        return _______________
```

Answer: `node.end - node.start + 1` - Calculate the length of an edge for internal nodes where end is explicitly set. For leaf nodes, end is None and we use the global leaf_end. This is part of Ukkonen's linear-time suffix tree construction algorithm.

#@@@@@@@@@@

83. What is the purpose of amortized analysis?

Answer: Amortized analysis provides average-case time complexity over a sequence of operations, not worst-case for individual operations. Methods include aggregate analysis, accounting method, and potential method. Used for data structures like dynamic arrays, splay trees, and Union-Find with path compression.

#@@@@@@@@@@

84. Which of the following are valid load balancing algorithms? (Multiple correct)
A) Round Robin
B) Least Connections
C) Weighted Round Robin
D) IP Hash
E) Least Response Time
F) Random

Answer: A, B, C, D, E, F - All are valid load balancing algorithms. Round Robin distributes requests sequentially, Least Connections routes to server with fewest active connections, Weighted Round Robin considers server capacity, IP Hash ensures session persistence, Least Response Time considers server performance, Random distributes randomly.

#@@@@@@@@@@

85. Find the bug in this Knuth-Morris-Pratt (KMP) algorithm:
```python
def compute_lps(pattern):
    """Compute Longest Proper Prefix which is also Suffix array"""
    m = len(pattern)
    lps = [0] * m
    length = 0
    i = 1

    while i < m:
        if pattern[i] == pattern[length]:
            length += 1
            lps[i] = length
            i += 1
        else:
            if length != 0:
                length = lps[length - 1]
            else:
                lps[i] = 0
                i += 1

    return lps

def kmp_search(text, pattern):
    n = len(text)
    m = len(pattern)

    if m == 0:
        return []

    lps = compute_lps(pattern)
    matches = []

    i = 0  # index for text
    j = 0  # index for pattern

    while i < n:
        if pattern[j] == text[i]:
            i += 1
            j += 1

        if j == m:
            matches.append(i - j)
            j = lps[j - 1]
        elif i < n and pattern[j] != text[i]:
            if j != 0:
                j = lps[j - 1]
            else:
                i += 1

    return matches

# Test
text = "ABABDABACDABABCABCABCABCABC"
pattern = "ABABCABCABCABC"
print(kmp_search(text, pattern))
```

Answer: No bug in the implementation. The KMP algorithm is correctly implemented with proper LPS array computation and pattern matching. It efficiently finds all occurrences of pattern in text with O(n+m) time complexity, avoiding redundant comparisons using the failure function.

#@@@@@@@@@@

86. How do you implement the Chinese Remainder Theorem?

Answer: CRT solves system of congruences x ≡ a₁ (mod m₁), x ≡ a₂ (mod m₂), ... where moduli are pairwise coprime. Compute M = m₁×m₂×...×mₖ, Mᵢ = M/mᵢ, find yᵢ such that Mᵢ×yᵢ ≡ 1 (mod mᵢ), then x = Σ(aᵢ×Mᵢ×yᵢ) mod M.

#@@@@@@@@@@

87. Which of the following are properties of consistent hashing? (Multiple correct)
A) Minimizes key redistribution when nodes change
B) Uses hash ring topology
C) Provides load balancing
D) Supports virtual nodes
E) O(1) lookup time
F) Fault tolerant

Answer: A, B, C, D, F - Consistent hashing minimizes redistribution, uses ring topology, provides load balancing with virtual nodes, and is fault tolerant. Lookup time is O(log n) with binary search on sorted ring, not O(1) unless using additional data structures.

#@@@@@@@@@@

88. Complete this parallel merge sort implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor

def merge(left, right):
    result = []
    i = j = 0

    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1

    result.extend(left[i:])
    result.extend(right[j:])
    return result

def parallel_merge_sort(arr, max_workers=4):
    if len(arr) <= 1:
        return arr

    # Base case for small arrays - use sequential sort
    if len(arr) < 1000:
        return sorted(arr)

    mid = len(arr) // 2

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit parallel tasks
        left_future = executor.submit(parallel_merge_sort, arr[:mid], max_workers)
        right_future = executor.submit(parallel_merge_sort, arr[mid:], max_workers)

        # Get results
        left_sorted = _______________
        right_sorted = right_future.result()

    return merge(left_sorted, right_sorted)

# Test
import random
arr = [random.randint(1, 1000) for _ in range(10000)]
sorted_arr = parallel_merge_sort(arr)
print(f"Sorted: {sorted_arr[:10]}...")  # Print first 10 elements
```

Answer: `left_future.result()` - Get the result from the future object representing the left half sorting task. ThreadPoolExecutor allows parallel execution of the recursive calls, improving performance on multi-core systems.

#@@@@@@@@@@

89. What is the difference between Las Vegas and Monte Carlo algorithms?

Answer: Las Vegas algorithms always produce correct results but have random running time (e.g., randomized quicksort). Monte Carlo algorithms have deterministic running time but may produce incorrect results with some probability (e.g., primality testing). Both use randomization but with different trade-offs.

#@@@@@@@@@@

90. Which of the following are applications of computational geometry? (Multiple correct)
A) Computer graphics and rendering
B) Geographic Information Systems (GIS)
C) Robotics path planning
D) Computer-aided design (CAD)
E) Image processing
F) Collision detection in games

Answer: A, B, C, D, E, F - All are computational geometry applications. Graphics (rendering, clipping), GIS (spatial queries), robotics (obstacle avoidance), CAD (geometric modeling), image processing (shape analysis), games (collision detection, visibility).

#@@@@@@@@@@

91. Predict the output of this divide-and-conquer algorithm:
```python
def count_inversions(arr):
    def merge_and_count(arr, temp, left, mid, right):
        i, j, k = left, mid + 1, left
        inv_count = 0

        while i <= mid and j <= right:
            if arr[i] <= arr[j]:
                temp[k] = arr[i]
                i += 1
            else:
                temp[k] = arr[j]
                inv_count += (mid - i + 1)  # All elements from i to mid are greater than arr[j]
                j += 1
            k += 1

        # Copy remaining elements
        while i <= mid:
            temp[k] = arr[i]
            i += 1
            k += 1
        while j <= right:
            temp[k] = arr[j]
            j += 1
            k += 1

        # Copy back to original array
        for i in range(left, right + 1):
            arr[i] = temp[i]

        return inv_count

    def merge_sort_and_count(arr, temp, left, right):
        inv_count = 0
        if left < right:
            mid = (left + right) // 2
            inv_count += merge_sort_and_count(arr, temp, left, mid)
            inv_count += merge_sort_and_count(arr, temp, mid + 1, right)
            inv_count += merge_and_count(arr, temp, left, mid, right)
        return inv_count

    temp = [0] * len(arr)
    return merge_sort_and_count(arr, temp, 0, len(arr) - 1)

arr = [8, 4, 2, 1]
print(count_inversions(arr))
```

Answer: 6 - The inversions are: (8,4), (8,2), (8,1), (4,2), (4,1), (2,1). This algorithm counts inversions using modified merge sort in O(n log n) time, counting how many elements in the left half are greater than elements in the right half during merging.

#@@@@@@@@@@

92. How do you implement the Miller-Rabin primality test?

Answer: Miller-Rabin is probabilistic primality test. Write n-1 = 2^r × d where d is odd. For random base a, compute a^d mod n, then repeatedly square r times. If any result is n-1, n is probably prime. If final result is 1 and no intermediate result was n-1, n is composite.

#@@@@@@@@@@

93. Which of the following are characteristics of cache-oblivious algorithms? (Multiple correct)
A) Optimal for all cache levels simultaneously
B) Don't need to know cache parameters
C) Use divide-and-conquer approach
D) Achieve optimal cache complexity
E) Work well on modern memory hierarchies
F) Always faster than cache-aware algorithms

Answer: A, B, C, D, E - Cache-oblivious algorithms are optimal for all cache levels without knowing parameters, typically use divide-and-conquer, achieve optimal cache complexity, and work well with memory hierarchies. They're not always faster than cache-aware algorithms but are more portable.

#@@@@@@@@@@

94. Find the issue in this external sorting algorithm:
```python
import heapq
import tempfile
import os

def external_sort(input_file, output_file, memory_limit=1000000):
    """Sort large file that doesn't fit in memory"""

    # Phase 1: Create sorted runs
    temp_files = []
    with open(input_file, 'r') as f:
        buffer = []

        for line in f:
            buffer.append(int(line.strip()))

            if len(buffer) >= memory_limit:
                # Sort buffer and write to temp file
                buffer.sort()
                temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False)
                for num in buffer:
                    temp_file.write(f"{num}\n")
                temp_file.close()
                temp_files.append(temp_file.name)
                buffer = []

        # Handle remaining buffer
        if buffer:
            buffer.sort()
            temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False)
            for num in buffer:
                temp_file.write(f"{num}\n")
            temp_file.close()
            temp_files.append(temp_file.name)

    # Phase 2: Merge sorted runs
    with open(output_file, 'w') as output:
        # Open all temp files
        file_handles = [open(f, 'r') for f in temp_files]
        heap = []

        # Initialize heap with first element from each file
        for i, fh in enumerate(file_handles):
            line = fh.readline()
            if line:
                heapq.heappush(heap, (int(line.strip()), i))

        # Merge using heap
        while heap:
            value, file_index = heapq.heappop(heap)
            output.write(f"{value}\n")

            # Read next element from same file
            line = file_handles[file_index].readline()
            if line:
                heapq.heappush(heap, (int(line.strip()), file_index))

        # Close all file handles
        for fh in file_handles:
            fh.close()

    # Clean up temp files
    for temp_file in temp_files:
        os.unlink(temp_file)

# Usage would be:
# external_sort('large_input.txt', 'sorted_output.txt')
```

Answer: No major bug in the algorithm logic. The external sort correctly implements the two-phase approach: create sorted runs that fit in memory, then merge using a min-heap. Potential improvements: error handling, memory usage optimization, and handling edge cases like empty files.

#@@@@@@@@@@

95. What is the time complexity of the Aho-Corasick string matching algorithm?

Answer: O(n + m + z) where n is text length, m is total length of all patterns, and z is number of matches. The algorithm builds a trie with failure links, enabling efficient multi-pattern matching. Preprocessing: O(m), searching: O(n + z), making it optimal for multiple pattern matching.

#@@@@@@@@@@

96. Which of the following are characteristics of Van Emde Boas trees? (Multiple correct)
A) Support operations in O(log log u) time
B) Use recursive structure
C) Require universe size to be power of 2
D) Support predecessor/successor queries
E) Use O(u) space
F) Faster than balanced BSTs for large universes

Answer: A, B, C, D, F - Van Emde Boas trees support operations in O(log log u) time where u is universe size, use recursive structure, require u to be power of 2, support predecessor/successor efficiently, and can be faster than BSTs. Space is O(u) which can be large.

#@@@@@@@@@@

97. Complete this Johnson's algorithm for all-pairs shortest paths:
```python
def johnsons_algorithm(graph):
    # Add new vertex s connected to all vertices with weight 0
    vertices = list(graph.keys())
    extended_graph = graph.copy()
    s = 'source'
    extended_graph[s] = {v: 0 for v in vertices}

    # Run Bellman-Ford from s to get h values
    h = bellman_ford(extended_graph, s)
    if h is None:
        return None  # Negative cycle detected

    # Reweight edges: w'(u,v) = w(u,v) + h(u) - h(v)
    reweighted_graph = {}
    for u in vertices:
        reweighted_graph[u] = {}
        for v, weight in graph[u].items():
            reweighted_graph[u][v] = weight + h[u] - h[v]

    # Run Dijkstra from each vertex
    all_pairs_distances = {}
    for u in vertices:
        distances = dijkstra(reweighted_graph, u)
        all_pairs_distances[u] = {}
        for v in vertices:
            if distances[v] != float('inf'):
                # Restore original weights: d(u,v) = d'(u,v) - h(u) + h(v)
                all_pairs_distances[u][v] = distances[v] - h[u] + _______________
            else:
                all_pairs_distances[u][v] = float('inf')

    return all_pairs_distances
```

Answer: `h[v]` - Restore the original distance by subtracting the reweighting. Johnson's algorithm combines Bellman-Ford and Dijkstra to handle negative weights efficiently with O(V²log V + VE) time complexity.

#@@@@@@@@@@

98. What is the difference between online and offline algorithms for the k-server problem?

Answer: Online k-server algorithms must serve requests without knowing future requests, measured by competitive ratio against optimal offline solution. Offline algorithms know all requests in advance and can compute optimal solution. The k-server conjecture states that there exists a k-competitive online algorithm.

#@@@@@@@@@@

99. Find the bug in this persistent data structure implementation:
```python
class PersistentArray:
    def __init__(self, data=None):
        if data is None:
            self.data = []
        else:
            self.data = data[:]  # Create copy

    def get(self, index):
        return self.data[index]

    def set(self, index, value):
        # Create new version with updated value
        new_data = self.data[:]  # Copy entire array - inefficient for large arrays
        new_data[index] = value
        return PersistentArray(new_data)

    def append(self, value):
        new_data = self.data[:]  # Copy entire array - inefficient
        new_data.append(value)
        return PersistentArray(new_data)

# Usage
arr1 = PersistentArray([1, 2, 3])
arr2 = arr1.set(1, 5)  # O(n) operation instead of O(log n)
arr3 = arr2.append(4)  # O(n) operation instead of O(log n)
```

Answer: The implementation is correct but inefficient. Copying the entire array for each operation gives O(n) time and space complexity. Should use structural sharing with trees (like tries or finger trees) to achieve O(log n) operations while maintaining persistence.

#@@@@@@@@@@

100. How do you implement the Karatsuba multiplication algorithm?

Answer: Karatsuba multiplies two n-digit numbers in O(n^log₂3) ≈ O(n^1.585) time using divide-and-conquer. Split numbers into halves, compute three products instead of four: ac, bd, and (a+b)(c+d), then combine using the identity xy = ac×10^n + ((a+b)(c+d) - ac - bd)×10^(n/2) + bd.

#@@@@@@@@@@

101. Which of the following are properties of locality-sensitive hashing? (Multiple correct)
A) Similar items hash to same buckets with high probability
B) Dissimilar items hash to different buckets with high probability
C) Used for approximate nearest neighbor search
D) Reduces dimensionality
E) Preserves distance relationships approximately
F) Always gives exact results

Answer: A, B, C, D, E - LSH ensures similar items collide with high probability, dissimilar items don't, used for ANN search, reduces dimensionality, and approximately preserves distances. It doesn't give exact results - it's a probabilistic approximation technique.

#@@@@@@@@@@

102. Complete this rope data structure implementation:
```python
class RopeNode:
    def __init__(self, data=None, left=None, right=None):
        self.data = data  # String data for leaf nodes
        self.left = left
        self.right = right
        self.weight = 0  # Length of left subtree

        if self.is_leaf():
            self.weight = len(data) if data else 0
        elif left:
            self.weight = left.total_length()

    def is_leaf(self):
        return self.left is None and self.right is None

    def total_length(self):
        if self.is_leaf():
            return len(self.data) if self.data else 0
        return self.weight + (self.right.total_length() if self.right else 0)

class Rope:
    def __init__(self, data=""):
        if len(data) <= 8:  # Leaf threshold
            self.root = RopeNode(data)
        else:
            mid = len(data) // 2
            left_rope = Rope(data[:mid])
            right_rope = Rope(data[mid:])
            self.root = RopeNode(left=left_rope.root, right=right_rope.root)
            self.root.weight = left_rope.root.total_length()

    def char_at(self, index):
        return self._char_at(self.root, index)

    def _char_at(self, node, index):
        if node.is_leaf():
            return node.data[index] if index < len(node.data) else None

        if index < node.weight:
            return self._char_at(node.left, index)
        else:
            return self._char_at(node.right, index - _______________)

    def concat(self, other_rope):
        new_root = RopeNode(left=self.root, right=other_rope.root)
        new_root.weight = self.root.total_length()
        result = Rope()
        result.root = new_root
        return result
```

Answer: `node.weight` - When searching in the right subtree, subtract the weight (length of left subtree) from the index. Ropes provide efficient string concatenation and substring operations with O(log n) complexity for most operations.

#@@@@@@@@@@

103. What is the purpose of the heavy-light decomposition technique?

Answer: Heavy-light decomposition decomposes a tree into heavy and light edges, creating at most O(log n) light edges on any root-to-leaf path. This enables efficient path queries and updates on trees in O(log² n) time by reducing tree problems to sequence problems that can be solved with segment trees.

#@@@@@@@@@@

104. Which of the following are applications of the Fast Walsh-Hadamard Transform? (Multiple correct)
A) Subset sum convolution
B) Boolean function analysis
C) Error-correcting codes
D) Cryptography
E) Signal processing
F) Polynomial multiplication over GF(2)

Answer: A, B, C, D, E, F - FWHT has applications in subset sum convolution, analyzing Boolean functions, error-correcting codes (Reed-Muller), cryptographic analysis, signal processing, and polynomial operations over finite fields. It's the discrete version of the Walsh-Hadamard transform.

#@@@@@@@@@@

105. Find the issue in this link-cut tree implementation:
```python
class LCTNode:
    def __init__(self, value):
        self.value = value
        self.parent = None
        self.left = None
        self.right = None
        self.reversed = False
        self.size = 1

    def is_root(self):
        return self.parent is None or (self.parent.left != self and self.parent.right != self)

    def push(self):
        if self.reversed:
            self.left, self.right = self.right, self.left
            if self.left:
                self.left.reversed = not self.left.reversed
            if self.right:
                self.right.reversed = not self.right.reversed
            self.reversed = False

    def update(self):
        self.size = 1
        if self.left:
            self.size += self.left.size
        if self.right:
            self.size += self.right.size

class LinkCutTree:
    def __init__(self):
        pass

    def access(self, node):
        # Make node the root of its auxiliary tree
        splay(node)
        node.right = None
        node.update()

        while node.parent:
            parent = node.parent
            splay(parent)
            parent.right = node
            parent.update()
            splay(node)  # Missing: need to maintain auxiliary tree structure

    def link(self, u, v):
        # Link u as child of v
        self.access(u)
        self.access(v)
        u.parent = v

    def cut(self, u, v):
        # Cut edge between u and v
        self.access(u)
        if u.left == v:
            u.left = None
            v.parent = None
            u.update()

def splay(node):
    # Simplified splay operation - full implementation needed
    while not node.is_root():
        if node.parent.is_root():
            # Zig step
            if node.parent.left == node:
                rotate_right(node.parent)
            else:
                rotate_left(node.parent)
        else:
            # Zig-zig or zig-zag steps
            pass  # Full splay implementation needed
```

Answer: The implementation is incomplete and has several issues: 1) Splay operation is not fully implemented, 2) Missing rotation functions, 3) Access operation doesn't properly maintain auxiliary tree invariants, 4) Missing push operations for lazy propagation, 5) Cut operation is oversimplified. Link-cut trees are complex data structures requiring careful implementation of splay trees with auxiliary tree maintenance.

#@@@@@@@@@@

106. How do you implement the Pollard's rho algorithm for integer factorization?

Answer: Pollard's rho uses Floyd's cycle detection on the sequence x_{i+1} = f(x_i) mod n where f(x) = x² + c. Find cycle using tortoise-hare method, compute gcd(|x_i - x_{2i}|, n) to find non-trivial factor. Expected time: O(n^{1/4}), much faster than trial division for large numbers.

#@@@@@@@@@@

107. Which of the following are characteristics of treaps? (Multiple correct)
A) Combination of binary search tree and heap
B) Use random priorities
C) Expected O(log n) operations
D) Self-balancing without rotations
E) Support split and merge operations
F) Deterministic structure

Answer: A, B, C, E - Treaps combine BST property (by keys) with heap property (by random priorities), have expected O(log n) operations, and support efficient split/merge. They do use rotations for balancing and have randomized (not deterministic) structure based on random priorities.

#@@@@@@@@@@

108. Complete this centroid decomposition algorithm:
```python
def centroid_decomposition(graph):
    n = len(graph)
    removed = [False] * n

    def get_subtree_size(node, parent):
        size = 1
        for neighbor in graph[node]:
            if neighbor != parent and not removed[neighbor]:
                size += get_subtree_size(neighbor, node)
        return size

    def find_centroid(node, parent, tree_size):
        for neighbor in graph[node]:
            if neighbor != parent and not removed[neighbor]:
                subtree_size = get_subtree_size(neighbor, node)
                if subtree_size > tree_size // 2:
                    return find_centroid(neighbor, node, tree_size)
        return node

    def decompose(node):
        tree_size = get_subtree_size(node, -1)
        centroid = find_centroid(node, -1, tree_size)

        # Process centroid (e.g., answer queries involving this centroid)
        process_centroid(centroid)

        # Mark centroid as removed
        removed[centroid] = True

        # Recursively decompose each subtree
        for neighbor in graph[centroid]:
            if not removed[neighbor]:
                _______________

    decompose(0)

def process_centroid(centroid):
    # Process queries or computations involving this centroid
    print(f"Processing centroid: {centroid}")
```

Answer: `decompose(neighbor)` - Recursively decompose each subtree after removing the centroid. Centroid decomposition creates a tree of centroids with O(log n) depth, enabling efficient path queries and tree algorithms with O(log n) complexity per query.

#@@@@@@@@@@

109. What is the difference between amortized and worst-case analysis?

Answer: Worst-case analysis considers the maximum time for any single operation. Amortized analysis considers the average time per operation over a sequence of operations, allowing some operations to be expensive if balanced by many cheap operations. Examples: dynamic array resize (amortized O(1), worst-case O(n)).

#@@@@@@@@@@

110. Which of the following problems can be solved using network flow algorithms? (Multiple correct)
A) Maximum bipartite matching
B) Minimum vertex cover
C) Maximum independent set
D) Minimum cut
E) Multi-commodity flow
F) Assignment problem

Answer: A, B, C, D, E, F - All can be solved using network flow. Bipartite matching (max flow), vertex cover (min cut), independent set (complement of vertex cover), min cut (max flow min cut theorem), multi-commodity flow (multiple flows), assignment problem (min cost max flow).

#@@@@@@@@@@

111. Predict the output of this geometric algorithm:
```python
def convex_hull_area(points):
    def cross_product(o, a, b):
        return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])

    # Sort points lexicographically
    points = sorted(set(points))
    if len(points) <= 1:
        return 0

    # Build lower hull
    lower = []
    for p in points:
        while len(lower) >= 2 and cross_product(lower[-2], lower[-1], p) <= 0:
            lower.pop()
        lower.append(p)

    # Build upper hull
    upper = []
    for p in reversed(points):
        while len(upper) >= 2 and cross_product(upper[-2], upper[-1], p) <= 0:
            upper.pop()
        upper.append(p)

    # Remove last point of each half because it's repeated
    hull = lower[:-1] + upper[:-1]

    # Calculate area using shoelace formula
    area = 0
    n = len(hull)
    for i in range(n):
        j = (i + 1) % n
        area += hull[i][0] * hull[j][1]
        area -= hull[j][0] * hull[i][1]

    return abs(area) / 2

points = [(0, 0), (4, 0), (4, 3), (0, 3), (2, 1)]
print(convex_hull_area(points))
```

Answer: 12.0 - The convex hull of the points forms a rectangle with vertices (0,0), (4,0), (4,3), (0,3). The area is 4 × 3 = 12. The point (2,1) is inside the convex hull and doesn't affect the area calculation.

#@@@@@@@@@@

112. How do you implement the Viterbi algorithm for Hidden Markov Models?

Answer: Viterbi uses dynamic programming to find most likely sequence of hidden states. Create DP table where dp[t][s] = max probability of being in state s at time t. For each time step, compute dp[t][s] = max over all previous states of (dp[t-1][prev] × transition[prev][s] × emission[s][observation[t]]). Backtrack to find optimal path.

#@@@@@@@@@@

113. Which of the following are properties of cuckoo hashing? (Multiple correct)
A) Guarantees O(1) worst-case lookup time
B) Uses two hash functions
C) May require rehashing during insertion
D) Achieves high load factor
E) Simple deletion operation
F) Always succeeds in insertion

Answer: A, B, C, E - Cuckoo hashing guarantees O(1) worst-case lookup using two hash functions, may need rehashing if cycles occur during insertion, and has simple deletion. It doesn't achieve very high load factors (typically ~50%) and insertion can fail requiring rehashing with new hash functions.

#@@@@@@@@@@

114. Complete this suffix automaton construction:
```python
class SuffixAutomatonNode:
    def __init__(self):
        self.transitions = {}
        self.suffix_link = None
        self.length = 0
        self.is_terminal = False

class SuffixAutomaton:
    def __init__(self):
        self.nodes = []
        self.last = 0

        # Create initial state
        initial = SuffixAutomatonNode()
        self.nodes.append(initial)

    def add_character(self, char):
        # Create new state
        new_node = SuffixAutomatonNode()
        new_node.length = self.nodes[self.last].length + 1
        self.nodes.append(new_node)
        new_state = len(self.nodes) - 1

        # Add transitions from states that don't have transition on char
        current = self.last
        while current != -1 and char not in self.nodes[current].transitions:
            self.nodes[current].transitions[char] = new_state
            current = self.nodes[current].suffix_link if self.nodes[current].suffix_link else -1

        if current == -1:
            # No state has transition on char
            new_node.suffix_link = 0
        else:
            next_state = self.nodes[current].transitions[char]
            if self.nodes[next_state].length == self.nodes[current].length + 1:
                # Direct suffix link
                new_node.suffix_link = next_state
            else:
                # Need to split state
                split_node = SuffixAutomatonNode()
                split_node.length = self.nodes[current].length + 1
                split_node.transitions = self.nodes[next_state].transitions.copy()
                split_node.suffix_link = self.nodes[next_state].suffix_link
                self.nodes.append(split_node)
                split_state = len(self.nodes) - 1

                # Update suffix links
                self.nodes[next_state].suffix_link = split_state
                new_node.suffix_link = split_state

                # Update transitions
                while current != -1 and self.nodes[current].transitions.get(char) == next_state:
                    self.nodes[current].transitions[char] = split_state
                    current = self.nodes[current].suffix_link if self.nodes[current].suffix_link else _______________

        self.last = new_state

    def build(self, text):
        for char in text:
            self.add_character(char)

        # Mark terminal states
        current = self.last
        while current != -1:
            self.nodes[current].is_terminal = True
            current = self.nodes[current].suffix_link if self.nodes[current].suffix_link else -1
```

Answer: `-1` - When there's no suffix link (for the root state), use -1 to indicate termination. Suffix automaton is a compressed trie of all suffixes, built incrementally in linear time, useful for string matching and counting distinct substrings.

#@@@@@@@@@@

115. What is the purpose of the fractional cascading technique?

Answer: Fractional cascading speeds up binary search across multiple sorted lists. Instead of performing separate binary searches in O(k log n) time, it preprocesses the lists to enable searching in O(log n + k) time by maintaining bridges between consecutive lists, allowing results to cascade from one search to the next.

#@@@@@@@@@@

116. Which of the following are valid approaches for the closest pair of points problem? (Multiple correct)
A) Brute force O(n²)
B) Divide and conquer O(n log n)
C) Sweep line algorithm O(n log n)
D) Randomized algorithm O(n log n)
E) Voronoi diagram approach O(n log n)
F) Grid-based approach O(n)

Answer: A, B, C, E - Brute force checks all pairs, divide-and-conquer splits points and merges results, sweep line processes points by x-coordinate, Voronoi diagram finds closest pairs as adjacent sites. Grid-based approach can be O(n) for uniformly distributed points but O(n²) worst-case. Randomized algorithms exist but are more complex.

#@@@@@@@@@@

117. Find the bug in this persistent segment tree implementation:
```python
class PersistentSegmentTreeNode:
    def __init__(self, value=0, left=None, right=None):
        self.value = value
        self.left = left
        self.right = right

class PersistentSegmentTree:
    def __init__(self, arr):
        self.n = len(arr)
        self.versions = []

        # Build initial tree
        root = self.build(arr, 0, self.n - 1)
        self.versions.append(root)

    def build(self, arr, start, end):
        if start == end:
            return PersistentSegmentTreeNode(arr[start])

        mid = (start + end) // 2
        left_child = self.build(arr, start, mid)
        right_child = self.build(arr, mid + 1, end)

        return PersistentSegmentTreeNode(
            left_child.value + right_child.value,
            left_child,
            right_child
        )

    def update(self, version, index, new_value):
        return self.update_helper(self.versions[version], 0, self.n - 1, index, new_value)

    def update_helper(self, node, start, end, index, new_value):
        if start == end:
            # Create new leaf node
            return PersistentSegmentTreeNode(new_value)

        mid = (start + end) // 2
        if index <= mid:
            # Update left subtree, reuse right subtree
            new_left = self.update_helper(node.left, start, mid, index, new_value)
            return PersistentSegmentTreeNode(
                new_left.value + node.right.value,
                new_left,
                node.right  # Reuse existing right subtree
            )
        else:
            # Update right subtree, reuse left subtree
            new_right = self.update_helper(node.right, mid + 1, end, index, new_value)
            return PersistentSegmentTreeNode(
                node.left.value + new_right.value,
                node.left,  # Reuse existing left subtree
                new_right
            )

    def query(self, version, left, right):
        return self.query_helper(self.versions[version], 0, self.n - 1, left, right)

    def query_helper(self, node, start, end, left, right):
        if right < start or left > end:
            return 0
        if left <= start and end <= right:
            return node.value

        mid = (start + end) // 2
        left_sum = self.query_helper(node.left, start, mid, left, right)
        right_sum = self.query_helper(node.right, mid + 1, end, left, right)
        return left_sum + right_sum

# Usage
arr = [1, 2, 3, 4, 5]
pst = PersistentSegmentTree(arr)

# Create new version with update
new_root = pst.update(0, 2, 10)  # Bug: not storing the new version
pst.versions.append(new_root)  # Should be done automatically

print(pst.query(0, 0, 4))  # Original version: 15
print(pst.query(1, 0, 4))  # Updated version: 22
```

Answer: The update method doesn't automatically store the new version in the versions list. The user must manually append the returned root. The update method should either automatically store the new version or the interface should be clearer about this requirement. Also missing proper version management and bounds checking.

#@@@@@@@@@@

118. How do you implement the Bentley-Ottmann algorithm for line segment intersection?

Answer: Bentley-Ottmann uses sweep line algorithm with event queue (sorted by x-coordinate) and status structure (y-ordered active segments). Process events: segment start (add to status), segment end (remove from status), intersection (swap segments). Check for intersections between adjacent segments in status structure. Time: O((n+k)log n) where k is intersections.

#@@@@@@@@@@

119. Which of the following are characteristics of B+ trees? (Multiple correct)
A) All data stored in leaf nodes
B) Internal nodes store only keys
C) Leaf nodes are linked
D) Better for range queries than B-trees
E) Higher fanout than B-trees
F) Used in database indexes

Answer: A, B, C, D, E, F - B+ trees store all data in leaves, internal nodes have only keys (allowing higher fanout), leaves are linked for efficient range queries, better for sequential access than B-trees, achieve higher fanout, and are widely used in database systems for indexing.

#@@@@@@@@@@

120. Complete this parallel quicksort implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
import random

def parallel_quicksort(arr, max_workers=4, threshold=1000):
    if len(arr) <= 1:
        return arr

    # Use sequential sort for small arrays
    if len(arr) < threshold:
        return sorted(arr)

    # Choose random pivot
    pivot = random.choice(arr)

    # Partition array
    less = [x for x in arr if x < pivot]
    equal = [x for x in arr if x == pivot]
    greater = [x for x in arr if x > pivot]

    # Base case: if one partition is empty, avoid unnecessary parallelization
    if len(less) == 0 or len(greater) == 0:
        return sorted(arr)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit parallel tasks
        less_future = executor.submit(parallel_quicksort, less, max_workers, threshold)
        greater_future = executor.submit(parallel_quicksort, greater, max_workers, threshold)

        # Get results
        sorted_less = less_future.result()
        sorted_greater = _______________

    return sorted_less + equal + sorted_greater

# Test
import time
arr = [random.randint(1, 10000) for _ in range(50000)]

start_time = time.time()
sorted_arr = parallel_quicksort(arr.copy())
parallel_time = time.time() - start_time

start_time = time.time()
sequential_sorted = sorted(arr.copy())
sequential_time = time.time() - start_time

print(f"Parallel time: {parallel_time:.4f}s")
print(f"Sequential time: {sequential_time:.4f}s")
print(f"Speedup: {sequential_time/parallel_time:.2f}x")
```

Answer: `greater_future.result()` - Get the result from the future representing the greater partition sorting task. Parallel quicksort can achieve significant speedup on multi-core systems, especially for large arrays, though the overhead of thread creation limits benefits for small arrays.

#@@@@@@@@@@

121. What is the difference between deterministic and randomized select algorithms?

Answer: Deterministic select (median-of-medians) guarantees O(n) worst-case time by carefully choosing pivot to ensure good partitions. Randomized select (quickselect) has O(n) expected time but O(n²) worst-case. Deterministic has higher constant factors but better worst-case guarantees; randomized is simpler and faster in practice.

#@@@@@@@@@@

122. Which of the following are applications of the Chinese Remainder Theorem? (Multiple correct)
A) RSA cryptography
B) Parallel computation
C) Calendar calculations
D) Polynomial interpolation
E) Fast modular arithmetic
F) Error correction codes

Answer: A, B, C, D, E, F - CRT is used in RSA for faster decryption, parallel algorithms for independent modular computations, calendar systems (finding dates), Lagrange interpolation over finite fields, speeding up modular arithmetic with large moduli, and constructing error-correcting codes.

#@@@@@@@@@@

123. Find the issue in this dancing links implementation:
```python
class DancingLinksNode:
    def __init__(self, column=None):
        self.left = self
        self.right = self
        self.up = self
        self.down = self
        self.column = column
        self.row_id = -1

class DancingLinksColumn(DancingLinksNode):
    def __init__(self, name):
        super().__init__(self)
        self.size = 0
        self.name = name
        self.column = self

class DancingLinks:
    def __init__(self, matrix):
        self.header = DancingLinksNode()
        self.columns = []
        self.solution = []

        # Create column headers
        num_cols = len(matrix[0]) if matrix else 0
        for i in range(num_cols):
            col = DancingLinksColumn(f"Col{i}")
            self.columns.append(col)

            # Link column to header
            col.left = self.header.left
            col.right = self.header
            self.header.left.right = col
            self.header.left = col

        # Create matrix nodes
        for row_idx, row in enumerate(matrix):
            prev_node = None
            first_node = None

            for col_idx, val in enumerate(row):
                if val == 1:
                    node = DancingLinksNode(self.columns[col_idx])
                    node.row_id = row_idx

                    # Link vertically
                    node.up = self.columns[col_idx].up
                    node.down = self.columns[col_idx]
                    self.columns[col_idx].up.down = node
                    self.columns[col_idx].up = node
                    self.columns[col_idx].size += 1

                    # Link horizontally
                    if prev_node is None:
                        first_node = node
                        prev_node = node
                    else:
                        node.left = prev_node
                        node.right = first_node
                        prev_node.right = node
                        first_node.left = node
                        prev_node = node

    def cover(self, column):
        # Remove column from header list
        column.right.left = column.left
        column.left.right = column.right

        # Remove all rows that contain this column
        row = column.down
        while row != column:
            node = row.right
            while node != row:
                node.up.down = node.down
                node.down.up = node.up
                node.column.size -= 1
                node = node.right
            row = row.down

    def uncover(self, column):
        # Restore all rows that contain this column
        row = column.up
        while row != column:
            node = row.left
            while node != row:
                node.column.size += 1
                node.up.down = node
                node.down.up = node
                node = node.left
            row = row.up

        # Restore column to header list
        column.right.left = column
        column.left.right = column

    def search(self):
        # Choose column with minimum size (heuristic)
        if self.header.right == self.header:
            return True  # Solution found

        column = None
        min_size = float('inf')
        col = self.header.right
        while col != self.header:
            if col.size < min_size:
                min_size = col.size
                column = col
            col = col.right

        if column.size == 0:
            return False  # No solution

        self.cover(column)

        row = column.down
        while row != column:
            self.solution.append(row.row_id)

            # Cover all other columns in this row
            node = row.right
            while node != row:
                self.cover(node.column)
                node = node.right

            # Recursive search
            if self.search():
                return True

            # Backtrack: uncover all columns in this row
            node = row.left
            while node != row:
                self.uncover(node.column)
                node = node.left

            self.solution.pop()
            row = row.down

        self.uncover(column)
        return False

# Test with exact cover problem
matrix = [
    [1, 0, 0, 1, 0, 0, 1],
    [1, 0, 0, 1, 0, 0, 0],
    [0, 0, 0, 1, 1, 0, 1],
    [0, 0, 1, 0, 1, 1, 0],
    [0, 1, 1, 0, 0, 1, 1],
    [0, 1, 0, 0, 0, 0, 1]
]

dl = DancingLinks(matrix)
if dl.search():
    print(f"Solution found: {dl.solution}")
else:
    print("No solution exists")
```

Answer: No major bugs in the implementation. The dancing links algorithm is correctly implemented for solving exact cover problems. The code properly handles the four-way linking, cover/uncover operations, and backtracking search. Minor improvements could include better error handling and optimization of the column selection heuristic.

#@@@@@@@@@@

124. How do you implement the Gale-Shapley algorithm for stable matching?

Answer: Gale-Shapley solves stable marriage problem. Men propose to women in order of preference. Women tentatively accept best proposal so far, rejecting others. Rejected men propose to next choice. Continue until all matched. Algorithm guarantees stable matching in O(n²) time, always terminates, and produces male-optimal solution.

#@@@@@@@@@@

125. Which of the following are properties of Fibonacci heaps? (Multiple correct)
A) Support decrease-key in O(1) amortized time
B) Support merge in O(1) time
C) Extract-min in O(log n) amortized time
D) Insert in O(1) time
E) Better constants than binary heaps
F) Used in Dijkstra's algorithm optimization

Answer: A, B, C, D, F - Fibonacci heaps support decrease-key and merge in O(1) amortized time, extract-min in O(log n) amortized, insert in O(1), and are used to optimize Dijkstra's algorithm. However, they have worse constants than binary heaps and are mainly theoretical improvements for dense graphs.

#@@@@@@@@@@

126. Predict the output of this string algorithm:
```python
def z_algorithm(s):
    n = len(s)
    z = [0] * n
    l, r = 0, 0

    for i in range(1, n):
        if i <= r:
            z[i] = min(r - i + 1, z[i - l])

        while i + z[i] < n and s[z[i]] == s[i + z[i]]:
            z[i] += 1

        if i + z[i] - 1 > r:
            l, r = i, i + z[i] - 1

    return z

s = "abcabcabc"
result = z_algorithm(s)
print(result)
```

Answer: [0, 0, 0, 6, 0, 0, 3, 0, 0] - Z-algorithm computes Z[i] = length of longest substring starting from i which is also prefix of string. At index 3: "abcabc" matches prefix "abcabc" (length 6). At index 6: "abc" matches prefix "abc" (length 3).

#@@@@@@@@@@

127. How do you implement the Hopcroft-Karp algorithm for maximum bipartite matching?

Answer: Hopcroft-Karp finds maximum bipartite matching in O(E√V) time. Use BFS to find augmenting paths of shortest length, then DFS to find all such paths simultaneously. Alternate between BFS phase (build level graph) and DFS phase (find augmenting paths) until no more augmenting paths exist.

#@@@@@@@@@@

128. Which of the following are characteristics of splay trees? (Multiple correct)
A) Self-adjusting binary search tree
B) Recently accessed elements move to root
C) O(log n) amortized time for operations
D) No explicit balance information stored
E) Good for temporal locality
F) Worst-case O(log n) for single operation

Answer: A, B, C, D, E - Splay trees are self-adjusting, move accessed elements to root via splaying, have O(log n) amortized time, store no balance info, and exploit temporal locality. Single operations can be O(n) worst-case, but amortized analysis guarantees O(log n).

#@@@@@@@@@@

129. Complete this parallel merge algorithm:
```python
import threading
from concurrent.futures import ThreadPoolExecutor

def parallel_merge(left, right, max_workers=4):
    if len(left) + len(right) < 1000:  # Threshold for sequential merge
        return sequential_merge(left, right)

    # Find median of combined arrays
    total_len = len(left) + len(right)
    median_pos = total_len // 2

    # Binary search to find split points
    left_split = binary_search_split(left, right, median_pos)
    right_split = median_pos - left_split

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Merge first halves in parallel
        first_half_future = executor.submit(
            parallel_merge,
            left[:left_split],
            right[:right_split],
            max_workers
        )

        # Merge second halves in parallel
        second_half_future = executor.submit(
            parallel_merge,
            left[left_split:],
            right[right_split:],
            max_workers
        )

        # Get results
        first_half = first_half_future.result()
        second_half = _______________

    return first_half + second_half

def sequential_merge(left, right):
    result = []
    i = j = 0

    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1

    result.extend(left[i:])
    result.extend(right[j:])
    return result

def binary_search_split(left, right, target_pos):
    # Find position in left array such that
    # left[:pos] + right[:target_pos-pos] has target_pos elements
    low, high = 0, min(len(left), target_pos)

    while low < high:
        mid = (low + high + 1) // 2
        right_pos = target_pos - mid

        if right_pos < 0:
            high = mid - 1
        elif right_pos > len(right):
            low = mid
        elif mid == len(left) or right_pos == 0 or left[mid-1] <= right[right_pos-1]:
            if right_pos == len(right) or mid == 0 or right[right_pos] >= left[mid]:
                return mid
            else:
                low = mid
        else:
            high = mid - 1

    return low
```

Answer: `second_half_future.result()` - Get the result from the future representing the second half merge task. Parallel merge can achieve speedup by recursively dividing the merge operation, though the complexity of finding optimal split points makes it more complex than parallel sorting algorithms.

#@@@@@@@@@@

130. What is the purpose of the heavy-hitter problem in streaming algorithms?

Answer: Heavy-hitter problem identifies elements that appear frequently in a data stream (frequency > threshold). Used in network monitoring, web analytics, database query optimization. Algorithms like Count-Min Sketch, Count Sketch, and Misra-Gries provide approximate solutions with sublinear space complexity.

#@@@@@@@@@@

131. Which of the following are valid approaches for the edit distance problem? (Multiple correct)
A) Dynamic programming O(mn)
B) Space-optimized DP O(min(m,n))
C) Hirschberg's algorithm O(mn) time, O(min(m,n)) space
D) Myers' algorithm O(nd) where d is edit distance
E) Four Russians technique O(mn/log n)
F) Suffix tree approach O(n+m)

Answer: A, B, C, D, E - All except F are valid. Standard DP is O(mn), can be space-optimized to O(min(m,n)), Hirschberg's combines divide-and-conquer with DP for optimal space, Myers' algorithm is faster for small edit distances, Four Russians uses bit parallelism. Suffix trees don't directly solve edit distance.

#@@@@@@@@@@

132. Find the bug in this interval tree implementation:
```python
class IntervalTreeNode:
    def __init__(self, interval):
        self.interval = interval  # (start, end)
        self.max_end = interval[1]
        self.left = None
        self.right = None

class IntervalTree:
    def __init__(self):
        self.root = None

    def insert(self, interval):
        self.root = self._insert(self.root, interval)

    def _insert(self, node, interval):
        if node is None:
            return IntervalTreeNode(interval)

        # Insert based on start time
        if interval[0] < node.interval[0]:
            node.left = self._insert(node.left, interval)
        else:
            node.right = self._insert(node.right, interval)

        # Update max_end
        node.max_end = max(node.max_end, interval[1])
        if node.left:
            node.max_end = max(node.max_end, node.left.max_end)
        if node.right:
            node.max_end = max(node.max_end, node.right.max_end)

        return node

    def search_overlapping(self, query_interval):
        result = []
        self._search_overlapping(self.root, query_interval, result)
        return result

    def _search_overlapping(self, node, query_interval, result):
        if node is None:
            return

        # Check if current interval overlaps with query
        if self._overlaps(node.interval, query_interval):
            result.append(node.interval)

        # Search left subtree if it might contain overlapping intervals
        if node.left and node.left.max_end >= query_interval[0]:
            self._search_overlapping(node.left, query_interval, result)

        # Search right subtree
        if node.right:
            self._search_overlapping(node.right, query_interval, result)

    def _overlaps(self, interval1, interval2):
        return interval1[0] <= interval2[1] and interval2[0] <= interval1[1]

# Test
tree = IntervalTree()
intervals = [(15, 20), (10, 30), (17, 19), (5, 20), (12, 15), (30, 40)]
for interval in intervals:
    tree.insert(interval)

# Search for overlapping intervals
query = (14, 16)
overlapping = tree.search_overlapping(query)
print(f"Intervals overlapping with {query}: {overlapping}")
```

Answer: No explicit bug in the implementation. The interval tree correctly maintains max_end values, inserts intervals based on start time, and searches for overlapping intervals efficiently. The search optimization using max_end values is properly implemented to prune unnecessary subtree searches.

#@@@@@@@@@@

133. How do you implement the Edmonds-Karp algorithm for maximum flow?

Answer: Edmonds-Karp is Ford-Fulkerson with BFS to find augmenting paths. Use BFS to find shortest augmenting path (in terms of number of edges), find bottleneck capacity along path, update residual graph. Repeat until no augmenting path exists. Time complexity: O(VE²), better than generic Ford-Fulkerson.

#@@@@@@@@@@

134. Which of the following are properties of locality-sensitive hashing families? (Multiple correct)
A) (r, cr, p1, p2)-sensitive for some c > 1
B) Pr[h(x) = h(y)] ≥ p1 if d(x,y) ≤ r
C) Pr[h(x) = h(y)] ≤ p2 if d(x,y) > cr
D) p1 > p2 for useful families
E) Used in approximate nearest neighbor search
F) Amplification improves separation

Answer: A, B, C, D, E, F - All are properties of LSH families. (r, cr, p1, p2)-sensitive means nearby points (distance ≤ r) collide with probability ≥ p1, far points (distance > cr) collide with probability ≤ p2, where p1 > p2. Used for ANN search with amplification techniques.

#@@@@@@@@@@

135. Complete this wavelet tree construction:
```python
class WaveletTreeNode:
    def __init__(self, alphabet_start, alphabet_end, sequence=None):
        self.alphabet_start = alphabet_start
        self.alphabet_end = alphabet_end
        self.bitmap = []
        self.left = None
        self.right = None

        if alphabet_start == alphabet_end:
            # Leaf node
            self.bitmap = [1] * len(sequence) if sequence else []
        else:
            # Internal node
            mid = (alphabet_start + alphabet_end) // 2
            left_seq = []
            right_seq = []

            for char in sequence:
                if char <= mid:
                    self.bitmap.append(0)
                    left_seq.append(char)
                else:
                    self.bitmap.append(1)
                    right_seq.append(char)

            if left_seq:
                self.left = WaveletTreeNode(alphabet_start, mid, left_seq)
            if right_seq:
                self.right = WaveletTreeNode(mid + 1, alphabet_end, right_seq)

class WaveletTree:
    def __init__(self, sequence):
        if not sequence:
            self.root = None
            return

        alphabet_start = min(sequence)
        alphabet_end = max(sequence)
        self.root = WaveletTreeNode(alphabet_start, alphabet_end, sequence)

    def rank(self, char, pos):
        """Count occurrences of char in sequence[0:pos]"""
        return self._rank(self.root, char, pos)

    def _rank(self, node, char, pos):
        if node is None or pos <= 0:
            return 0

        if node.alphabet_start == node.alphabet_end:
            return pos if node.alphabet_start == char else 0

        mid = (node.alphabet_start + node.alphabet_end) // 2

        if char <= mid:
            # Count zeros up to position pos, then recurse left
            zeros_before_pos = sum(1 for i in range(pos) if node.bitmap[i] == 0)
            return self._rank(node.left, char, zeros_before_pos)
        else:
            # Count ones up to position pos, then recurse right
            ones_before_pos = sum(1 for i in range(pos) if node.bitmap[i] == 1)
            return self._rank(node.right, char, _______________)

    def access(self, pos):
        """Get character at position pos"""
        return self._access(self.root, pos)

    def _access(self, node, pos):
        if node.alphabet_start == node.alphabet_end:
            return node.alphabet_start

        if node.bitmap[pos] == 0:
            # Go left
            zeros_before_pos = sum(1 for i in range(pos) if node.bitmap[i] == 0)
            return self._access(node.left, zeros_before_pos)
        else:
            # Go right
            ones_before_pos = sum(1 for i in range(pos) if node.bitmap[i] == 0)
            return self._access(node.right, ones_before_pos)

# Test
sequence = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3]
wt = WaveletTree(sequence)
print(f"Character at position 4: {wt.access(4)}")  # Should be 5
print(f"Rank of 1 up to position 4: {wt.rank(1, 4)}")  # Should be 2
```

Answer: `ones_before_pos` - When recursing to the right child, pass the count of ones (1s) encountered before position pos. Wavelet trees provide efficient rank and select operations on sequences over large alphabets using O(n log σ) space where σ is alphabet size.

#@@@@@@@@@@

136. What is the difference between online and offline minimum spanning tree algorithms?

Answer: Online MST algorithms maintain MST as edges are added/deleted dynamically, supporting queries about connectivity and MST weight. Offline algorithms (Kruskal's, Prim's) process all edges at once to build static MST. Online algorithms are more complex but handle dynamic graphs; offline algorithms are simpler and more efficient for static graphs.

#@@@@@@@@@@

137. Which of the following are applications of the Fast Fourier Transform? (Multiple correct)
A) Polynomial multiplication
B) Convolution computation
C) Signal processing
D) Image compression
E) Solving linear recurrences
F) String matching

Answer: A, B, C, D, E, F - FFT has wide applications: polynomial multiplication (reduce from O(n²) to O(n log n)), convolution (signal processing), frequency domain analysis, image/audio compression (DCT), solving linear recurrences with constant coefficients, and string matching algorithms.

#@@@@@@@@@@

138. Find the issue in this red-black tree implementation:
```python
class RBNode:
    def __init__(self, data, color='RED'):
        self.data = data
        self.color = color
        self.left = None
        self.right = None
        self.parent = None

class RedBlackTree:
    def __init__(self):
        self.NIL = RBNode(None, 'BLACK')
        self.root = self.NIL

    def insert(self, data):
        new_node = RBNode(data)
        new_node.left = self.NIL
        new_node.right = self.NIL

        parent = None
        current = self.root

        # Find insertion position
        while current != self.NIL:
            parent = current
            if new_node.data < current.data:
                current = current.left
            else:
                current = current.right

        new_node.parent = parent

        if parent is None:
            self.root = new_node
        elif new_node.data < parent.data:
            parent.left = new_node
        else:
            parent.right = new_node

        # Fix red-black tree properties
        self.insert_fixup(new_node)

    def insert_fixup(self, node):
        while node.parent and node.parent.color == 'RED':
            if node.parent == node.parent.parent.left:
                uncle = node.parent.parent.right

                if uncle.color == 'RED':
                    # Case 1: Uncle is red
                    node.parent.color = 'BLACK'
                    uncle.color = 'BLACK'
                    node.parent.parent.color = 'RED'
                    node = node.parent.parent
                else:
                    if node == node.parent.right:
                        # Case 2: Node is right child
                        node = node.parent
                        self.left_rotate(node)

                    # Case 3: Node is left child
                    node.parent.color = 'BLACK'
                    node.parent.parent.color = 'RED'
                    self.right_rotate(node.parent.parent)
            else:
                # Symmetric cases for right subtree
                uncle = node.parent.parent.left

                if uncle.color == 'RED':
                    node.parent.color = 'BLACK'
                    uncle.color = 'BLACK'
                    node.parent.parent.color = 'RED'
                    node = node.parent.parent
                else:
                    if node == node.parent.left:
                        node = node.parent
                        self.right_rotate(node)

                    node.parent.color = 'BLACK'
                    node.parent.parent.color = 'RED'
                    self.left_rotate(node.parent.parent)

        self.root.color = 'BLACK'  # Root is always black

    def left_rotate(self, x):
        y = x.right
        x.right = y.left

        if y.left != self.NIL:
            y.left.parent = x

        y.parent = x.parent

        if x.parent is None:
            self.root = y
        elif x == x.parent.left:
            x.parent.left = y
        else:
            x.parent.right = y

        y.left = x
        x.parent = y

    def right_rotate(self, x):
        y = x.left
        x.left = y.right

        if y.right != self.NIL:
            y.right.parent = x

        y.parent = x.parent

        if x.parent is None:
            self.root = y
        elif x == x.parent.right:
            x.parent.right = y
        else:
            x.parent.left = y

        y.right = x
        x.parent = y

# Test
rbt = RedBlackTree()
values = [10, 20, 30, 15, 25, 5, 1]
for val in values:
    rbt.insert(val)
```

Answer: No major bugs in the implementation. The red-black tree correctly implements insertion with proper fixup, rotation operations, and maintains red-black properties. The NIL sentinel node is properly used, and the fixup cases handle all scenarios correctly. Minor improvements could include deletion operation and better error handling.

#@@@@@@@@@@

139. How do you implement the Needleman-Wunsch algorithm for global sequence alignment?

Answer: Needleman-Wunsch uses dynamic programming for optimal global alignment. Create matrix where dp[i][j] = optimal score for aligning first i characters of sequence1 with first j characters of sequence2. Recurrence: dp[i][j] = max(dp[i-1][j-1] + match/mismatch, dp[i-1][j] + gap, dp[i][j-1] + gap). Backtrack to find alignment.

#@@@@@@@@@@

140. Which of the following are characteristics of count-min sketch? (Multiple correct)
A) Probabilistic data structure
B) Estimates frequency of elements
C) Uses multiple hash functions
D) Can overestimate but not underestimate
E) Sublinear space complexity
F) Supports deletions

Answer: A, B, C, D, E - Count-min sketch is probabilistic, estimates frequencies using multiple hash functions, can overestimate (due to hash collisions) but never underestimate, uses sublinear space. Standard count-min sketch doesn't support deletions (would need conservative update or other modifications).

#@@@@@@@@@@

141. Complete this parallel radix sort implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor

def parallel_radix_sort(arr, max_workers=4):
    if not arr:
        return arr

    # Find maximum number to determine number of digits
    max_num = max(arr)
    num_digits = len(str(max_num))

    # Sort by each digit position
    for digit_pos in range(num_digits):
        arr = parallel_counting_sort_by_digit(arr, digit_pos, max_workers)

    return arr

def parallel_counting_sort_by_digit(arr, digit_pos, max_workers):
    n = len(arr)
    if n < 1000:  # Use sequential for small arrays
        return counting_sort_by_digit(arr, digit_pos)

    # Divide array into chunks for parallel processing
    chunk_size = n // max_workers
    chunks = [arr[i:i + chunk_size] for i in range(0, n, chunk_size)]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Count digits in each chunk in parallel
        count_futures = [
            executor.submit(count_digits_in_chunk, chunk, digit_pos)
            for chunk in chunks
        ]

        # Collect counts from all chunks
        chunk_counts = [future.result() for future in count_futures]

    # Merge counts from all chunks
    total_counts = [0] * 10
    for counts in chunk_counts:
        for i in range(10):
            total_counts[i] += counts[i]

    # Calculate cumulative positions
    positions = [0] * 10
    for i in range(1, 10):
        positions[i] = positions[i-1] + total_counts[i-1]

    # Place elements in sorted order
    result = [0] * n
    for num in arr:
        digit = (num // (10 ** digit_pos)) % 10
        result[positions[digit]] = num
        positions[digit] += _______________

    return result

def count_digits_in_chunk(chunk, digit_pos):
    counts = [0] * 10
    for num in chunk:
        digit = (num // (10 ** digit_pos)) % 10
        counts[digit] += 1
    return counts

def counting_sort_by_digit(arr, digit_pos):
    counts = [0] * 10

    # Count occurrences of each digit
    for num in arr:
        digit = (num // (10 ** digit_pos)) % 10
        counts[digit] += 1

    # Calculate positions
    positions = [0] * 10
    for i in range(1, 10):
        positions[i] = positions[i-1] + counts[i-1]

    # Place elements
    result = [0] * len(arr)
    for num in arr:
        digit = (num // (10 ** digit_pos)) % 10
        result[positions[digit]] = num
        positions[digit] += 1

    return result

# Test
import random
arr = [random.randint(1, 10000) for _ in range(50000)]
sorted_arr = parallel_radix_sort(arr.copy())
print(f"First 10 elements: {sorted_arr[:10]}")
print(f"Last 10 elements: {sorted_arr[-10:]}")
print(f"Is sorted: {sorted_arr == sorted(arr)}")
```

Answer: `1` - Increment the position by 1 after placing each element to maintain the correct position for the next element with the same digit. Parallel radix sort can achieve good speedup by parallelizing the counting phase while maintaining the stability of the sort.

#@@@@@@@@@@

142. What is the purpose of the Burrows-Wheeler Transform in data compression?

Answer: BWT rearranges characters to group similar characters together, improving compressibility. It's reversible and often used with move-to-front coding and Huffman coding in compression algorithms like bzip2. BWT creates runs of identical characters that compress well, while maintaining the ability to perfectly reconstruct the original string.

#@@@@@@@@@@

143. Which of the following are valid approaches for the longest common substring problem? (Multiple correct)
A) Dynamic programming O(mn)
B) Suffix array with LCP O(n log n)
C) Suffix tree O(n+m)
D) Rolling hash O(nm)
E) Binary search + hashing O(n log n)
F) KMP algorithm O(n+m)

Answer: A, B, C, E - DP builds table tracking common suffixes, suffix array with LCP finds longest common substring efficiently, suffix tree provides linear solution, binary search with hashing searches for substring of specific length. Rolling hash alone doesn't solve LCS, and KMP finds pattern occurrences, not longest common substring.

#@@@@@@@@@@

144. Find the bug in this Bloom filter implementation:
```python
import hashlib
import math

class BloomFilter:
    def __init__(self, capacity, error_rate):
        self.capacity = capacity
        self.error_rate = error_rate

        # Calculate optimal parameters
        self.size = int(-capacity * math.log(error_rate) / (math.log(2) ** 2))
        self.hash_count = int(self.size * math.log(2) / capacity)

        # Initialize bit array
        self.bit_array = [False] * self.size
        self.item_count = 0

    def _hash(self, item, seed):
        """Generate hash value for item with given seed"""
        hasher = hashlib.md5()
        hasher.update(f"{item}{seed}".encode('utf-8'))
        return int(hasher.hexdigest(), 16) % self.size

    def add(self, item):
        """Add item to bloom filter"""
        for i in range(self.hash_count):
            index = self._hash(item, i)
            self.bit_array[index] = True
        self.item_count += 1

    def contains(self, item):
        """Check if item might be in the set"""
        for i in range(self.hash_count):
            index = self._hash(item, i)
            if not self.bit_array[index]:
                return False
        return True

    def current_error_rate(self):
        """Estimate current false positive rate"""
        if self.item_count == 0:
            return 0.0

        # Probability that a bit is still 0
        prob_zero = (1 - 1/self.size) ** (self.hash_count * self.item_count)

        # Probability of false positive
        prob_false_positive = (1 - prob_zero) ** self.hash_count
        return prob_false_positive

# Test
bf = BloomFilter(1000, 0.01)

# Add some items
items = ["apple", "banana", "cherry", "date", "elderberry"]
for item in items:
    bf.add(item)

# Test membership
print(f"Contains 'apple': {bf.contains('apple')}")  # Should be True
print(f"Contains 'grape': {bf.contains('grape')}")  # Should be False (probably)
print(f"Current error rate: {bf.current_error_rate():.4f}")

# Test with many items to see if error rate increases
for i in range(100):
    bf.add(f"item_{i}")

print(f"Error rate after adding more items: {bf.current_error_rate():.4f}")
```

Answer: No explicit bug in the implementation. The Bloom filter correctly calculates optimal parameters, uses multiple hash functions, and implements add/contains operations properly. The error rate calculation is mathematically correct. Minor improvements could include better hash function distribution and handling of capacity overflow.

#@@@@@@@@@@

145. How do you implement the Earley parser for context-free grammars?

Answer: Earley parser uses dynamic programming with three operations: Predictor (adds new states for non-terminals), Scanner (advances states that match input), Completer (handles completed rules). Maintains chart with states for each input position. Each state contains rule, dot position, and origin. Time: O(n³) general, O(n²) unambiguous, O(n) deterministic grammars.

#@@@@@@@@@@

146. Which of the following are characteristics of persistent data structures? (Multiple correct)
A) Preserve previous versions after updates
B) Support efficient structural sharing
C) Immutable operations
D) Copy-on-write semantics
E) Higher memory usage than mutable structures
F) Thread-safe by design

Answer: A, B, C, D, F - Persistent structures preserve old versions, use structural sharing for efficiency, provide immutable operations, implement copy-on-write, and are inherently thread-safe. Memory usage can be higher but structural sharing minimizes overhead compared to naive copying.

#@@@@@@@@@@

147. Complete this parallel matrix multiplication:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
import numpy as np

def parallel_matrix_multiply(A, B, max_workers=4, block_size=64):
    rows_A, cols_A = len(A), len(A[0])
    rows_B, cols_B = len(B), len(B[0])

    if cols_A != rows_B:
        raise ValueError("Matrix dimensions don't match for multiplication")

    # Initialize result matrix
    C = [[0] * cols_B for _ in range(rows_A)]

    # Use block multiplication for cache efficiency
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []

        # Divide work into blocks
        for i in range(0, rows_A, block_size):
            for j in range(0, cols_B, block_size):
                for k in range(0, cols_A, block_size):
                    # Submit block multiplication task
                    future = executor.submit(
                        multiply_block,
                        A, B, C,
                        i, j, k,
                        min(i + block_size, rows_A),
                        min(j + block_size, cols_B),
                        min(k + block_size, cols_A)
                    )
                    futures.append(future)

        # Wait for all tasks to complete
        for future in futures:
            _______________

    return C

def multiply_block(A, B, C, start_i, start_j, start_k, end_i, end_j, end_k):
    """Multiply a block of matrices A and B, accumulating in C"""
    for i in range(start_i, end_i):
        for j in range(start_j, end_j):
            for k in range(start_k, end_k):
                C[i][j] += A[i][k] * B[k][j]

# Test with large matrices
def create_random_matrix(rows, cols):
    return [[np.random.randint(1, 10) for _ in range(cols)] for _ in range(rows)]

# Test
size = 512
A = create_random_matrix(size, size)
B = create_random_matrix(size, size)

import time
start_time = time.time()
C_parallel = parallel_matrix_multiply(A, B)
parallel_time = time.time() - start_time

print(f"Parallel multiplication time: {parallel_time:.4f}s")
print(f"Result shape: {len(C_parallel)}x{len(C_parallel[0])}")

# Verify correctness with small matrices
A_small = [[1, 2], [3, 4]]
B_small = [[5, 6], [7, 8]]
C_small = parallel_matrix_multiply(A_small, B_small)
print(f"Small matrix result: {C_small}")  # Should be [[19, 22], [43, 50]]
```

Answer: `future.result()` - Wait for each task to complete by calling result() on the future. This ensures all block multiplications finish before returning the final result matrix. Parallel block matrix multiplication can achieve significant speedup on multi-core systems while improving cache locality.

#@@@@@@@@@@

148. What is the difference between deterministic and non-deterministic finite automata?

Answer: DFA has exactly one transition for each symbol from each state, while NFA can have multiple transitions or epsilon transitions. Every NFA can be converted to equivalent DFA using subset construction, but DFA may have exponentially more states. NFAs are more convenient for design, DFAs are more efficient for implementation.

#@@@@@@@@@@

149. Which of the following are applications of the maximum flow problem? (Multiple correct)
A) Image segmentation
B) Airline scheduling
C) Bipartite matching
D) Network reliability
E) Project selection
F) Baseball elimination

Answer: A, B, C, D, E, F - All are max flow applications. Image segmentation (min cut), airline scheduling (capacity constraints), bipartite matching (unit capacity), network reliability (connectivity), project selection (profit maximization), baseball elimination (determining if team can still win).

#@@@@@@@@@@

150. Predict the output of this computational geometry algorithm:
```python
def point_in_polygon(point, polygon):
    """Ray casting algorithm to determine if point is inside polygon"""
    x, y = point
    n = len(polygon)
    inside = False

    p1x, p1y = polygon[0]
    for i in range(1, n + 1):
        p2x, p2y = polygon[i % n]

        if y > min(p1y, p2y):
            if y <= max(p1y, p2y):
                if x <= max(p1x, p2x):
                    if p1y != p2y:
                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                    if p1x == p2x or x <= xinters:
                        inside = not inside

        p1x, p1y = p2x, p2y

    return inside

# Test with square polygon
square = [(0, 0), (4, 0), (4, 4), (0, 4)]
test_points = [(2, 2), (5, 5), (0, 0), (2, 0)]

for point in test_points:
    result = point_in_polygon(point, square)
    print(f"Point {point} in polygon: {result}")
```

Answer:
Point (2, 2) in polygon: True
Point (5, 5) in polygon: False
Point (0, 0) in polygon: False
Point (2, 0) in polygon: False

The ray casting algorithm counts intersections of a horizontal ray from the point to infinity. Odd count means inside, even count means outside. Points on boundary are typically considered outside in this implementation.

#@@@@@@@@@@

151. How do you implement the Cooley-Tukey FFT algorithm?

Answer: Cooley-Tukey FFT uses divide-and-conquer with decimation-in-time. Recursively split DFT into even and odd indexed elements, compute smaller DFTs, then combine using twiddle factors. Base case: DFT of single element is the element itself. Time complexity: O(n log n) vs O(n²) for naive DFT.

#@@@@@@@@@@

152. Which of the following are characteristics of hash tables with linear probing? (Multiple correct)
A) Open addressing collision resolution
B) Primary clustering problem
C) Good cache locality
D) Simple deletion requires tombstones
E) Load factor affects performance significantly
F) Worst-case O(1) operations

Answer: A, B, C, D, E - Linear probing uses open addressing, suffers from primary clustering (consecutive occupied slots), has good cache locality due to sequential access, requires tombstones for deletion, and performance degrades with high load factor. Worst-case is O(n), not O(1).

#@@@@@@@@@@

153. Complete this parallel quickselect implementation:
```python
import random
import threading
from concurrent.futures import ThreadPoolExecutor

def parallel_quickselect(arr, k, max_workers=4, threshold=1000):
    """Find kth smallest element using parallel quickselect"""
    if len(arr) <= threshold:
        return sequential_quickselect(arr, k)

    # Choose random pivot
    pivot = random.choice(arr)

    # Partition array in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        chunk_size = len(arr) // max_workers
        chunks = [arr[i:i + chunk_size] for i in range(0, len(arr), chunk_size)]

        # Partition each chunk in parallel
        partition_futures = [
            executor.submit(partition_chunk, chunk, pivot)
            for chunk in chunks
        ]

        # Collect partitioned chunks
        partitioned_chunks = [future.result() for future in partition_futures]

    # Merge partitioned chunks
    less = []
    equal = []
    greater = []

    for chunk_less, chunk_equal, chunk_greater in partitioned_chunks:
        less.extend(chunk_less)
        equal.extend(chunk_equal)
        greater.extend(chunk_greater)

    # Determine which partition contains kth element
    if k < len(less):
        return parallel_quickselect(less, k, max_workers, threshold)
    elif k < len(less) + len(equal):
        return pivot
    else:
        return parallel_quickselect(greater, k - len(less) - len(equal), max_workers, threshold)

def partition_chunk(chunk, pivot):
    """Partition a chunk around pivot"""
    less = [x for x in chunk if x < pivot]
    equal = [x for x in chunk if x == pivot]
    greater = [x for x in chunk if x > pivot]
    return less, equal, greater

def sequential_quickselect(arr, k):
    """Sequential quickselect for small arrays"""
    if len(arr) == 1:
        return arr[0]

    pivot = random.choice(arr)
    less = [x for x in arr if x < pivot]
    equal = [x for x in arr if x == pivot]
    greater = [x for x in arr if x > pivot]

    if k < len(less):
        return sequential_quickselect(less, k)
    elif k < len(less) + len(equal):
        return _______________
    else:
        return sequential_quickselect(greater, k - len(less) - len(equal))

# Test
import time
arr = [random.randint(1, 100000) for _ in range(100000)]
k = 50000  # Find median

start_time = time.time()
result_parallel = parallel_quickselect(arr.copy(), k)
parallel_time = time.time() - start_time

start_time = time.time()
result_sequential = sequential_quickselect(arr.copy(), k)
sequential_time = time.time() - start_time

print(f"Parallel result: {result_parallel}")
print(f"Sequential result: {result_sequential}")
print(f"Parallel time: {parallel_time:.4f}s")
print(f"Sequential time: {sequential_time:.4f}s")
print(f"Speedup: {sequential_time/parallel_time:.2f}x")
```

Answer: `pivot` - When k falls in the equal partition, return the pivot value since all elements in the equal partition have the same value as the pivot. Parallel quickselect can achieve speedup by parallelizing the partitioning phase.

#@@@@@@@@@@

154. What is the purpose of the van Emde Boas tree data structure?

Answer: vEB trees support predecessor, successor, insert, delete, and member operations in O(log log u) time where u is universe size. They use recursive structure with clusters and summary, achieving better performance than balanced BSTs for large integer universes. Useful when universe size is much larger than number of elements.

#@@@@@@@@@@

155. Which of the following are valid approaches for the all-pairs shortest paths problem? (Multiple correct)
A) Run Dijkstra from each vertex O(V²log V + VE)
B) Floyd-Warshall algorithm O(V³)
C) Johnson's algorithm O(V²log V + VE)
D) Matrix exponentiation O(V³log V)
E) Repeated squaring O(V³log V)
F) Bellman-Ford from each vertex O(V²E)

Answer: A, B, C, D, E, F - All are valid approaches. Dijkstra from each vertex (for non-negative weights), Floyd-Warshall (handles negative weights), Johnson's (reweighting + Dijkstra), matrix exponentiation (for unweighted graphs), repeated squaring (distance matrix), Bellman-Ford from each vertex (handles negative weights but slower).

#@@@@@@@@@@

156. Find the bug in this treap implementation:
```python
import random

class TreapNode:
    def __init__(self, key, priority=None):
        self.key = key
        self.priority = priority if priority is not None else random.random()
        self.left = None
        self.right = None

class Treap:
    def __init__(self):
        self.root = None

    def insert(self, key):
        self.root = self._insert(self.root, key)

    def _insert(self, node, key):
        if node is None:
            return TreapNode(key)

        if key < node.key:
            node.left = self._insert(node.left, key)
            # Check heap property and rotate if necessary
            if node.left.priority > node.priority:
                node = self._rotate_right(node)
        elif key > node.key:
            node.right = self._insert(node.right, key)
            # Check heap property and rotate if necessary
            if node.right.priority > node.priority:
                node = self._rotate_left(node)
        # If key == node.key, do nothing (no duplicates)

        return node

    def _rotate_left(self, x):
        y = x.right
        x.right = y.left
        y.left = x
        return y

    def _rotate_right(self, x):
        y = x.left
        x.left = y.right
        y.right = x
        return y

    def delete(self, key):
        self.root = self._delete(self.root, key)

    def _delete(self, node, key):
        if node is None:
            return None

        if key < node.key:
            node.left = self._delete(node.left, key)
        elif key > node.key:
            node.right = self._delete(node.right, key)
        else:
            # Found node to delete
            if node.left is None:
                return node.right
            elif node.right is None:
                return node.left
            else:
                # Both children exist - rotate node down
                if node.left.priority > node.right.priority:
                    node = self._rotate_right(node)
                    node.right = self._delete(node.right, key)
                else:
                    node = self._rotate_left(node)
                    node.left = self._delete(node.left, key)

        return node

    def search(self, key):
        return self._search(self.root, key)

    def _search(self, node, key):
        if node is None or node.key == key:
            return node

        if key < node.key:
            return self._search(node.left, key)
        else:
            return self._search(node.right, key)

# Test
treap = Treap()
keys = [10, 5, 15, 3, 7, 12, 18]

for key in keys:
    treap.insert(key)

# Test search
for key in [5, 15, 20]:
    result = treap.search(key)
    print(f"Search {key}: {'Found' if result else 'Not found'}")

# Test deletion
treap.delete(10)
result = treap.search(10)
print(f"Search 10 after deletion: {'Found' if result else 'Not found'}")
```

Answer: No explicit bug in the implementation. The treap correctly maintains both BST property (by keys) and heap property (by priorities) through rotations. Insert, delete, and search operations are properly implemented. The randomized priorities ensure expected O(log n) height and operations.

#@@@@@@@@@@

157. How do you implement the Manacher's algorithm for finding all palindromes?

Answer: Manacher's algorithm finds all palindromes in O(n) time. Transform string by inserting special characters between every character. Maintain center and right boundary of rightmost palindrome. For each position, use symmetry property to initialize palindrome length, then expand. Update center and boundary when palindrome extends beyond current boundary.

#@@@@@@@@@@

158. Which of the following are properties of cuckoo hashing? (Multiple correct)
A) Worst-case O(1) lookup time
B) Uses two hash tables
C) May require rehashing during insertion
D) High space utilization
E) Simple deletion operation
F) Guaranteed successful insertion

Answer: A, B, C, E - Cuckoo hashing guarantees O(1) worst-case lookup using two hash tables, may need rehashing if insertion fails due to cycles, and has simple deletion. Space utilization is typically around 50% (not high), and insertion can fail requiring rehashing with new hash functions.

#@@@@@@@@@@

159. Complete this parallel merge sort with work stealing:
```python
import threading
import queue
from concurrent.futures import ThreadPoolExecutor
import time

class WorkStealingMergeSort:
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.work_queues = [queue.Queue() for _ in range(max_workers)]
        self.results = {}
        self.task_id_counter = 0
        self.lock = threading.Lock()

    def sort(self, arr):
        if len(arr) <= 1:
            return arr

        # Submit initial task
        task_id = self._get_next_task_id()
        self.work_queues[0].put(('sort', task_id, arr, 0, len(arr) - 1))

        # Start worker threads
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._worker, worker_id)
                for worker_id in range(self.max_workers)
            ]

            # Wait for initial task to complete
            while task_id not in self.results:
                time.sleep(0.001)

            # Signal workers to stop
            for i in range(self.max_workers):
                self.work_queues[i].put(('stop', None, None, None, None))

            # Wait for all workers to finish
            for future in futures:
                future.result()

        return self.results[task_id]

    def _worker(self, worker_id):
        while True:
            # Try to get work from own queue
            try:
                task = self.work_queues[worker_id].get_nowait()
            except queue.Empty:
                # Try to steal work from other queues
                task = self._steal_work(worker_id)
                if task is None:
                    time.sleep(0.001)
                    continue

            task_type, task_id, arr, left, right = task

            if task_type == 'stop':
                break
            elif task_type == 'sort':
                self._process_sort_task(task_id, arr, left, right, worker_id)
            elif task_type == 'merge':
                self._process_merge_task(task_id, arr, left, right, worker_id)

    def _steal_work(self, worker_id):
        # Try to steal from other workers' queues
        for i in range(1, self.max_workers):
            victim_id = (worker_id + i) % self.max_workers
            try:
                return self.work_queues[victim_id].get_nowait()
            except queue.Empty:
                continue
        return None

    def _process_sort_task(self, task_id, arr, left, right, worker_id):
        if left >= right:
            self.results[task_id] = arr[left:right+1] if left == right else []
            return

        if right - left < 1000:  # Sequential threshold
            self.results[task_id] = sorted(arr[left:right+1])
            return

        # Divide into subtasks
        mid = (left + right) // 2

        left_task_id = self._get_next_task_id()
        right_task_id = self._get_next_task_id()
        merge_task_id = self._get_next_task_id()

        # Submit subtasks
        self.work_queues[worker_id].put(('sort', left_task_id, arr, left, mid))
        self.work_queues[worker_id].put(('sort', right_task_id, arr, mid + 1, right))
        self.work_queues[worker_id].put(('merge', merge_task_id, (left_task_id, right_task_id, task_id), left, right))

    def _process_merge_task(self, task_id, task_info, left, right, worker_id):
        left_task_id, right_task_id, parent_task_id = task_info

        # Wait for subtasks to complete
        while left_task_id not in self.results or right_task_id not in self.results:
            # Try to do other work while waiting
            try:
                other_task = self.work_queues[worker_id].get_nowait()
                if other_task[0] != 'stop':
                    task_type, other_task_id, other_arr, other_left, other_right = other_task
                    if task_type == 'sort':
                        self._process_sort_task(other_task_id, other_arr, other_left, other_right, worker_id)
                    elif task_type == 'merge':
                        self._process_merge_task(other_task_id, other_arr, other_left, other_right, worker_id)
            except queue.Empty:
                time.sleep(0.001)

        # Merge results
        left_result = self.results[left_task_id]
        right_result = self.results[right_task_id]
        merged = self._merge(left_result, right_result)
        self.results[_______________] = merged

    def _merge(self, left, right):
        result = []
        i = j = 0

        while i < len(left) and j < len(right):
            if left[i] <= right[j]:
                result.append(left[i])
                i += 1
            else:
                result.append(right[j])
                j += 1

        result.extend(left[i:])
        result.extend(right[j:])
        return result

    def _get_next_task_id(self):
        with self.lock:
            self.task_id_counter += 1
            return self.task_id_counter

# Test
import random
arr = [random.randint(1, 10000) for _ in range(50000)]

sorter = WorkStealingMergeSort()
start_time = time.time()
sorted_arr = sorter.sort(arr.copy())
parallel_time = time.time() - start_time

start_time = time.time()
sequential_sorted = sorted(arr.copy())
sequential_time = time.time() - start_time

print(f"Parallel time: {parallel_time:.4f}s")
print(f"Sequential time: {sequential_time:.4f}s")
print(f"Speedup: {sequential_time/parallel_time:.2f}x")
print(f"Correctly sorted: {sorted_arr == sequential_sorted}")
```

Answer: `parent_task_id` - Store the merged result under the parent task ID so the parent task can access the combined result. Work-stealing merge sort can achieve good load balancing by allowing idle workers to steal tasks from busy workers.

#@@@@@@@@@@

160. What is the difference between online and offline algorithms for the k-server problem?

Answer: Online k-server algorithms must serve requests without knowing future requests, measured by competitive ratio against optimal offline solution. Best known competitive ratio is 2k-1. Offline algorithms know all requests in advance and can compute optimal solution. The k-server conjecture states there exists a k-competitive online algorithm.

#@@@@@@@@@@

161. Which of the following are applications of the suffix array data structure? (Multiple correct)
A) String matching
B) Longest common prefix queries
C) Longest repeated substring
D) Burrows-Wheeler Transform
E) Suffix tree simulation
F) Pattern counting

Answer: A, B, C, D, E, F - Suffix arrays support all these applications: string matching (binary search), LCP queries (with LCP array), longest repeated substring (max LCP value), BWT construction, suffix tree simulation with LCP array, and pattern counting (range of matching suffixes).

#@@@@@@@@@@

162. Find the issue in this skip list implementation:
```python
import random

class SkipListNode:
    def __init__(self, key, value, level):
        self.key = key
        self.value = value
        self.forward = [None] * (level + 1)

class SkipList:
    def __init__(self, max_level=16, p=0.5):
        self.max_level = max_level
        self.p = p
        self.header = SkipListNode(float('-inf'), None, max_level)
        self.level = 0

    def random_level(self):
        level = 0
        while random.random() < self.p and level < self.max_level:
            level += 1
        return level

    def search(self, key):
        current = self.header

        # Start from highest level and go down
        for i in range(self.level, -1, -1):
            while current.forward[i] and current.forward[i].key < key:
                current = current.forward[i]

        # Move to next node at level 0
        current = current.forward[0]

        if current and current.key == key:
            return current.value
        return None

    def insert(self, key, value):
        update = [None] * (self.max_level + 1)
        current = self.header

        # Find insertion point and record update pointers
        for i in range(self.level, -1, -1):
            while current.forward[i] and current.forward[i].key < key:
                current = current.forward[i]
            update[i] = current

        current = current.forward[0]

        if current and current.key == key:
            # Update existing key
            current.value = value
        else:
            # Insert new node
            new_level = self.random_level()

            if new_level > self.level:
                for i in range(self.level + 1, new_level + 1):
                    update[i] = self.header
                self.level = new_level

            new_node = SkipListNode(key, value, new_level)

            for i in range(new_level + 1):
                new_node.forward[i] = update[i].forward[i]
                update[i].forward[i] = new_node

    def delete(self, key):
        update = [None] * (self.max_level + 1)
        current = self.header

        # Find deletion point and record update pointers
        for i in range(self.level, -1, -1):
            while current.forward[i] and current.forward[i].key < key:
                current = current.forward[i]
            update[i] = current

        current = current.forward[0]

        if current and current.key == key:
            # Remove node from all levels
            for i in range(self.level + 1):
                if update[i].forward[i] != current:
                    break
                update[i].forward[i] = current.forward[i]

            # Update skip list level
            while self.level > 0 and self.header.forward[self.level] is None:
                self.level -= 1

            return True
        return False

# Test
skip_list = SkipList()

# Insert some values
values = [(3, "three"), (6, "six"), (7, "seven"), (9, "nine"), (12, "twelve"), (19, "nineteen"), (17, "seventeen"), (26, "twenty-six"), (21, "twenty-one"), (25, "twenty-five")]

for key, value in values:
    skip_list.insert(key, value)

# Test search
for key in [6, 15, 19]:
    result = skip_list.search(key)
    print(f"Search {key}: {result}")

# Test deletion
print(f"Delete 6: {skip_list.delete(6)}")
print(f"Search 6 after deletion: {skip_list.search(6)}")
```

Answer: No major bugs in the implementation. The skip list correctly implements probabilistic balancing, search, insert, and delete operations. The random level generation, update pointer tracking, and level maintenance are properly handled. Minor improvements could include better error handling and iterator support.

#@@@@@@@@@@

163. How do you implement the Kosaraju's algorithm for strongly connected components?

Answer: Kosaraju's algorithm finds SCCs in two DFS passes: 1) Perform DFS on original graph, record finish times in stack. 2) Create transpose graph (reverse all edges). 3) Perform DFS on transpose graph in decreasing order of finish times. Each DFS tree in second pass is an SCC. Time: O(V+E).

#@@@@@@@@@@

164. Which of the following are characteristics of B* trees? (Multiple correct)
A) Variant of B+ trees
B) Higher minimum fill factor
C) Delayed splitting strategy
D) Better space utilization than B+ trees
E) More complex insertion algorithm
F) Used in file systems

Answer: A, B, C, D, E, F - B* trees are B+ tree variants with higher minimum fill factor (2/3 instead of 1/2), use delayed splitting by redistributing keys to siblings before splitting, achieve better space utilization, have more complex insertion due to redistribution, and are used in some file systems.

#@@@@@@@@@@

165. Complete this parallel bucket sort implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
import math

def parallel_bucket_sort(arr, max_workers=4, num_buckets=None):
    if not arr:
        return arr

    n = len(arr)
    if num_buckets is None:
        num_buckets = int(math.sqrt(n))

    # Find range of values
    min_val = min(arr)
    max_val = max(arr)
    range_val = max_val - min_val + 1

    # Create buckets
    buckets = [[] for _ in range(num_buckets)]

    # Distribute elements into buckets
    for num in arr:
        bucket_index = min(int((num - min_val) * num_buckets / range_val), num_buckets - 1)
        buckets[bucket_index].append(num)

    # Sort buckets in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit sorting tasks for non-empty buckets
        sort_futures = []
        for i, bucket in enumerate(buckets):
            if bucket:
                future = executor.submit(sorted, bucket)
                sort_futures.append((i, future))

        # Collect sorted buckets
        for i, future in sort_futures:
            buckets[i] = _______________

    # Concatenate sorted buckets
    result = []
    for bucket in buckets:
        result.extend(bucket)

    return result

def parallel_bucket_sort_advanced(arr, max_workers=4):
    """Advanced version with parallel distribution and sorting"""
    if not arr:
        return arr

    n = len(arr)
    num_buckets = max_workers * 4  # More buckets for better load balancing

    # Find range
    min_val = min(arr)
    max_val = max(arr)
    range_val = max_val - min_val + 1

    # Parallel distribution into buckets
    buckets = [[] for _ in range(num_buckets)]
    bucket_locks = [threading.Lock() for _ in range(num_buckets)]

    def distribute_chunk(chunk):
        for num in chunk:
            bucket_index = min(int((num - min_val) * num_buckets / range_val), num_buckets - 1)
            with bucket_locks[bucket_index]:
                buckets[bucket_index].append(num)

    # Divide array into chunks for parallel distribution
    chunk_size = max(1, n // max_workers)
    chunks = [arr[i:i + chunk_size] for i in range(0, n, chunk_size)]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Parallel distribution
        distribution_futures = [
            executor.submit(distribute_chunk, chunk)
            for chunk in chunks
        ]

        # Wait for distribution to complete
        for future in distribution_futures:
            future.result()

        # Parallel sorting of buckets
        sort_futures = []
        for i, bucket in enumerate(buckets):
            if bucket:
                future = executor.submit(sorted, bucket)
                sort_futures.append((i, future))

        # Collect sorted buckets
        for i, future in sort_futures:
            buckets[i] = future.result()

    # Concatenate results
    result = []
    for bucket in buckets:
        result.extend(bucket)

    return result

# Test
import random
import time

arr = [random.randint(1, 10000) for _ in range(100000)]

# Test basic parallel bucket sort
start_time = time.time()
sorted_arr1 = parallel_bucket_sort(arr.copy())
time1 = time.time() - start_time

# Test advanced parallel bucket sort
start_time = time.time()
sorted_arr2 = parallel_bucket_sort_advanced(arr.copy())
time2 = time.time() - start_time

# Test sequential sort
start_time = time.time()
sorted_arr3 = sorted(arr.copy())
time3 = time.time() - start_time

print(f"Basic parallel bucket sort time: {time1:.4f}s")
print(f"Advanced parallel bucket sort time: {time2:.4f}s")
print(f"Sequential sort time: {time3:.4f}s")
print(f"All results equal: {sorted_arr1 == sorted_arr2 == sorted_arr3}")
```

Answer: `future.result()` - Get the sorted bucket from the future object. Parallel bucket sort can achieve good speedup when the data is uniformly distributed across buckets, allowing independent sorting of each bucket.

#@@@@@@@@@@

166. What is the purpose of the Christofides algorithm?

Answer: Christofides algorithm provides a 1.5-approximation for the metric Traveling Salesman Problem. Steps: 1) Find MST, 2) Find vertices with odd degree, 3) Find minimum weight perfect matching on odd-degree vertices, 4) Combine MST and matching to form Eulerian graph, 5) Find Eulerian tour, 6) Convert to Hamiltonian tour by skipping repeated vertices.

#@@@@@@@@@@

167. Which of the following are valid approaches for the maximum subarray problem? (Multiple correct)
A) Kadane's algorithm O(n)
B) Divide and conquer O(n log n)
C) Brute force O(n²)
D) Dynamic programming O(n)
E) Prefix sum approach O(n)
F) Segment tree approach O(n log n)

Answer: A, B, C, D, E - Kadane's algorithm is optimal O(n), divide-and-conquer splits array and combines results, brute force checks all subarrays, DP is equivalent to Kadane's, prefix sums can find maximum subarray efficiently. Segment trees are overkill for this problem and don't directly apply.

#@@@@@@@@@@

168. Predict the output of this graph algorithm:
```python
def tarjan_scc(graph):
    """Tarjan's algorithm for strongly connected components"""
    n = len(graph)
    ids = [-1] * n
    low = [-1] * n
    on_stack = [False] * n
    stack = []
    id_counter = 0
    scc_count = 0

    def dfs(at):
        nonlocal id_counter, scc_count

        stack.append(at)
        on_stack[at] = True
        ids[at] = low[at] = id_counter
        id_counter += 1

        # Visit neighbors
        for to in graph[at]:
            if ids[to] == -1:
                dfs(to)
            if on_stack[to]:
                low[at] = min(low[at], low[to])

        # Found SCC root
        if ids[at] == low[at]:
            scc = []
            while True:
                node = stack.pop()
                on_stack[node] = False
                scc.append(node)
                if node == at:
                    break
            print(f"SCC {scc_count}: {scc}")
            scc_count += 1

    # Find all SCCs
    for i in range(n):
        if ids[i] == -1:
            dfs(i)

    return scc_count

# Test graph: 0->1->2->0, 3->4->3, 5
graph = [
    [1],      # 0 -> 1
    [2],      # 1 -> 2
    [0],      # 2 -> 0
    [4],      # 3 -> 4
    [3],      # 4 -> 3
    []        # 5 (isolated)
]

num_sccs = tarjan_scc(graph)
print(f"Total SCCs: {num_sccs}")
```

Answer:
SCC 0: [2, 1, 0]
SCC 1: [4, 3]
SCC 2: [5]
Total SCCs: 3

Tarjan's algorithm finds three strongly connected components: {0,1,2} form a cycle, {3,4} form a cycle, and {5} is isolated. The algorithm uses DFS with low-link values to identify SCC roots.

#@@@@@@@@@@

169. How do you implement the Booth's algorithm for string rotation?

Answer: Booth's algorithm finds lexicographically minimal string rotation in O(n) time. Concatenate string with itself, use failure function similar to KMP to find the starting position of minimal rotation. Compare characters and advance pointers based on comparison results, maintaining candidate starting positions.

#@@@@@@@@@@

170. Which of the following are characteristics of finger trees? (Multiple correct)
A) Support efficient concatenation
B) Provide O(1) access to ends
C) Support efficient splitting
D) Use 2-3 trees as building blocks
E) Persistent data structure
F) Support random access in O(log n)

Answer: A, B, C, D, E, F - Finger trees support efficient concatenation and splitting, provide O(1) access to both ends, use 2-3 trees internally, can be made persistent, and support O(log n) random access. They're versatile data structures used in functional programming.

#@@@@@@@@@@

171. Complete this parallel convex hull algorithm:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
import math

def parallel_convex_hull(points, max_workers=4):
    """Parallel divide-and-conquer convex hull algorithm"""
    if len(points) < 3:
        return points

    if len(points) < 1000:  # Sequential threshold
        return graham_scan(points)

    # Divide points
    mid = len(points) // 2
    left_points = points[:mid]
    right_points = points[mid:]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Compute convex hulls in parallel
        left_future = executor.submit(parallel_convex_hull, left_points, max_workers)
        right_future = executor.submit(parallel_convex_hull, right_points, max_workers)

        # Get results
        left_hull = left_future.result()
        right_hull = _______________

    # Merge the two convex hulls
    return merge_hulls(left_hull, right_hull)

def graham_scan(points):
    """Sequential Graham scan for convex hull"""
    if len(points) < 3:
        return points

    # Find bottom-most point (or leftmost in case of tie)
    start = min(points, key=lambda p: (p[1], p[0]))

    # Sort points by polar angle with respect to start point
    def polar_angle(p):
        dx = p[0] - start[0]
        dy = p[1] - start[1]
        return math.atan2(dy, dx)

    sorted_points = sorted([p for p in points if p != start], key=polar_angle)

    # Build convex hull
    hull = [start]

    for point in sorted_points:
        # Remove points that make clockwise turn
        while len(hull) > 1 and cross_product(hull[-2], hull[-1], point) <= 0:
            hull.pop()
        hull.append(point)

    return hull

def merge_hulls(left_hull, right_hull):
    """Merge two convex hulls"""
    # Find upper and lower tangents
    upper_tangent = find_upper_tangent(left_hull, right_hull)
    lower_tangent = find_lower_tangent(left_hull, right_hull)

    # Build merged hull
    merged_hull = []

    # Add points from left hull (from lower to upper tangent)
    left_start = lower_tangent[0]
    left_end = upper_tangent[0]

    i = left_start
    while True:
        merged_hull.append(left_hull[i])
        if i == left_end:
            break
        i = (i + 1) % len(left_hull)

    # Add points from right hull (from upper to lower tangent)
    right_start = upper_tangent[1]
    right_end = lower_tangent[1]

    i = right_start
    while True:
        merged_hull.append(right_hull[i])
        if i == right_end:
            break
        i = (i + 1) % len(right_hull)

    return merged_hull

def find_upper_tangent(left_hull, right_hull):
    """Find upper tangent between two convex hulls"""
    # Start with rightmost point of left hull and leftmost point of right hull
    left_idx = 0
    right_idx = 0

    # Find rightmost point in left hull
    for i in range(len(left_hull)):
        if left_hull[i][0] > left_hull[left_idx][0]:
            left_idx = i

    # Find leftmost point in right hull
    for i in range(len(right_hull)):
        if right_hull[i][0] < right_hull[right_idx][0]:
            right_idx = i

    # Find upper tangent
    while True:
        changed = False

        # Check if we can move left_idx clockwise
        next_left = (left_idx + 1) % len(left_hull)
        if cross_product(left_hull[left_idx], right_hull[right_idx], left_hull[next_left]) < 0:
            left_idx = next_left
            changed = True

        # Check if we can move right_idx counter-clockwise
        prev_right = (right_idx - 1) % len(right_hull)
        if cross_product(left_hull[left_idx], right_hull[right_idx], right_hull[prev_right]) > 0:
            right_idx = prev_right
            changed = True

        if not changed:
            break

    return (left_idx, right_idx)

def find_lower_tangent(left_hull, right_hull):
    """Find lower tangent between two convex hulls"""
    # Similar to upper tangent but with opposite direction
    left_idx = 0
    right_idx = 0

    # Find rightmost point in left hull
    for i in range(len(left_hull)):
        if left_hull[i][0] > left_hull[left_idx][0]:
            left_idx = i

    # Find leftmost point in right hull
    for i in range(len(right_hull)):
        if right_hull[i][0] < right_hull[right_idx][0]:
            right_idx = i

    # Find lower tangent
    while True:
        changed = False

        # Check if we can move left_idx counter-clockwise
        prev_left = (left_idx - 1) % len(left_hull)
        if cross_product(left_hull[left_idx], right_hull[right_idx], left_hull[prev_left]) > 0:
            left_idx = prev_left
            changed = True

        # Check if we can move right_idx clockwise
        next_right = (right_idx + 1) % len(right_hull)
        if cross_product(left_hull[left_idx], right_hull[right_idx], right_hull[next_right]) < 0:
            right_idx = next_right
            changed = True

        if not changed:
            break

    return (left_idx, right_idx)

def cross_product(o, a, b):
    """Calculate cross product of vectors OA and OB"""
    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])

# Test
import random
points = [(random.randint(0, 100), random.randint(0, 100)) for _ in range(1000)]

hull = parallel_convex_hull(points)
print(f"Convex hull has {len(hull)} points")
print(f"First few hull points: {hull[:5]}")
```

Answer: `right_future.result()` - Get the convex hull result from the right partition. Parallel convex hull algorithms can achieve speedup by dividing the point set and computing hulls in parallel, then merging the results using tangent finding algorithms.

#@@@@@@@@@@

172. What is the difference between Las Vegas and Monte Carlo randomized algorithms?

Answer: Las Vegas algorithms always produce correct results but have random running time (e.g., randomized quicksort with random pivot). Monte Carlo algorithms have deterministic or bounded running time but may produce incorrect results with some probability (e.g., Miller-Rabin primality test). Trade-off between correctness guarantee and time guarantee.

#@@@@@@@@@@

173. Which of the following are applications of the maximum bipartite matching problem? (Multiple correct)
A) Job assignment problem
B) Marriage matching
C) Network flow
D) Vertex cover in bipartite graphs
E) Edge coloring
F) Hall's theorem verification

Answer: A, B, C, D, E, F - All are applications: job assignment (workers to jobs), marriage matching (stable marriages), network flow (unit capacity), vertex cover (König's theorem), edge coloring (Vizing's theorem), and Hall's theorem provides necessary/sufficient conditions for perfect matching.

#@@@@@@@@@@

174. Find the bug in this disjoint set union implementation:
```python
class DisjointSetUnion:
    def __init__(self, n):
        self.parent = list(range(n))
        self.rank = [0] * n
        self.size = [1] * n
        self.components = n

    def find(self, x):
        """Find with path compression"""
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])
        return self.parent[x]

    def union(self, x, y):
        """Union by rank with size tracking"""
        root_x = self.find(x)
        root_y = self.find(y)

        if root_x == root_y:
            return False  # Already in same component

        # Union by rank
        if self.rank[root_x] < self.rank[root_y]:
            self.parent[root_x] = root_y
            self.size[root_y] += self.size[root_x]
        elif self.rank[root_x] > self.rank[root_y]:
            self.parent[root_y] = root_x
            self.size[root_x] += self.size[root_y]
        else:
            self.parent[root_y] = root_x
            self.size[root_x] += self.size[root_y]
            self.rank[root_x] += 1

        self.components -= 1
        return True

    def connected(self, x, y):
        """Check if x and y are in same component"""
        return self.find(x) == self.find(y)

    def component_size(self, x):
        """Get size of component containing x"""
        return self.size[self.find(x)]

    def num_components(self):
        """Get number of connected components"""
        return self.components

# Test
dsu = DisjointSetUnion(10)

# Perform some unions
unions = [(0, 1), (2, 3), (4, 5), (1, 2), (6, 7), (5, 6)]
for x, y in unions:
    result = dsu.union(x, y)
    print(f"Union({x}, {y}): {result}")

# Check connectivity
print(f"Connected(0, 3): {dsu.connected(0, 3)}")  # Should be True
print(f"Connected(0, 8): {dsu.connected(0, 8)}")  # Should be False

# Check component sizes
print(f"Component size of 0: {dsu.component_size(0)}")  # Should be 4
print(f"Component size of 5: {dsu.component_size(5)}")  # Should be 4
print(f"Total components: {dsu.num_components()}")      # Should be 4
```

Answer: No explicit bug in the implementation. The disjoint set union correctly implements path compression, union by rank, size tracking, and component counting. The operations maintain the invariants properly and provide the expected amortized O(α(n)) time complexity where α is the inverse Ackermann function.

#@@@@@@@@@@

175. How do you implement the Aho-Corasick algorithm for multiple pattern matching?

Answer: Aho-Corasick builds a trie of patterns, then adds failure links for efficient transitions when mismatches occur. Failure links point to longest proper suffix that's also a prefix. During search, follow failure links on mismatch instead of restarting. Output links identify all patterns ending at current state. Time: O(n + m + z) where z is matches.

#@@@@@@@@@@

176. Which of the following are characteristics of randomized quicksort? (Multiple correct)
A) Expected O(n log n) time complexity
B) Worst-case O(n²) time complexity
C) In-place sorting algorithm
D) Not stable
E) Good average-case performance
F) Deterministic pivot selection

Answer: A, B, C, D, E - Randomized quicksort has expected O(n log n) time, O(n²) worst-case, sorts in-place, is not stable, and performs well on average. It uses random pivot selection, not deterministic, which helps avoid worst-case behavior on already sorted arrays.

#@@@@@@@@@@

177. Complete this parallel breadth-first search implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
from collections import deque
import time

def parallel_bfs(graph, start, max_workers=4):
    """Parallel BFS using level-synchronous approach"""
    visited = set()
    distances = {start: 0}
    current_level = {start}
    level = 0

    visited.add(start)

    while current_level:
        next_level = set()

        # Process current level in parallel
        if len(current_level) > max_workers:
            # Divide current level into chunks
            current_list = list(current_level)
            chunk_size = len(current_list) // max_workers
            chunks = [current_list[i:i + chunk_size] for i in range(0, len(current_list), chunk_size)]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Process each chunk in parallel
                futures = [
                    executor.submit(process_bfs_chunk, chunk, graph, visited, distances, level + 1)
                    for chunk in chunks
                ]

                # Collect results from all chunks
                for future in futures:
                    chunk_next_level = _______________
                    next_level.update(chunk_next_level)
        else:
            # Sequential processing for small levels
            for node in current_level:
                for neighbor in graph[node]:
                    if neighbor not in visited:
                        visited.add(neighbor)
                        distances[neighbor] = level + 1
                        next_level.add(neighbor)

        current_level = next_level
        level += 1

    return distances

def process_bfs_chunk(chunk, graph, visited, distances, level):
    """Process a chunk of nodes in current BFS level"""
    chunk_next_level = set()
    visited_lock = threading.Lock()

    for node in chunk:
        for neighbor in graph[node]:
            with visited_lock:
                if neighbor not in visited:
                    visited.add(neighbor)
                    distances[neighbor] = level
                    chunk_next_level.add(neighbor)

    return chunk_next_level

def parallel_bfs_optimized(graph, start, max_workers=4):
    """Optimized parallel BFS with better load balancing"""
    visited = [False] * len(graph)
    distances = [-1] * len(graph)

    current_frontier = [start]
    visited[start] = True
    distances[start] = 0
    level = 0

    while current_frontier:
        next_frontier = []

        if len(current_frontier) > 100:  # Threshold for parallelization
            # Use thread-safe queue for next frontier
            next_frontier_queue = deque()
            frontier_lock = threading.Lock()

            def process_nodes(nodes):
                local_next = []
                for node in nodes:
                    for neighbor in graph[node]:
                        if not visited[neighbor]:
                            with frontier_lock:
                                if not visited[neighbor]:  # Double-check
                                    visited[neighbor] = True
                                    distances[neighbor] = level + 1
                                    local_next.append(neighbor)

                with frontier_lock:
                    next_frontier_queue.extend(local_next)

            # Divide frontier into chunks
            chunk_size = max(1, len(current_frontier) // max_workers)
            chunks = [current_frontier[i:i + chunk_size] for i in range(0, len(current_frontier), chunk_size)]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(process_nodes, chunk) for chunk in chunks]
                for future in futures:
                    future.result()

            next_frontier = list(next_frontier_queue)
        else:
            # Sequential processing
            for node in current_frontier:
                for neighbor in graph[node]:
                    if not visited[neighbor]:
                        visited[neighbor] = True
                        distances[neighbor] = level + 1
                        next_frontier.append(neighbor)

        current_frontier = next_frontier
        level += 1

    # Convert to dictionary format
    result = {}
    for i in range(len(distances)):
        if distances[i] != -1:
            result[i] = distances[i]

    return result

# Test
def create_test_graph(n, edges_per_node=3):
    """Create a test graph with n nodes"""
    import random
    graph = [[] for _ in range(n)]

    for i in range(n):
        # Add random edges
        for _ in range(edges_per_node):
            neighbor = random.randint(0, n - 1)
            if neighbor != i and neighbor not in graph[i]:
                graph[i].append(neighbor)
                graph[neighbor].append(i)  # Undirected graph

    return graph

# Performance test
n = 10000
graph = create_test_graph(n)

start_time = time.time()
distances_parallel = parallel_bfs(graph, 0)
parallel_time = time.time() - start_time

start_time = time.time()
distances_optimized = parallel_bfs_optimized(graph, 0)
optimized_time = time.time() - start_time

print(f"Parallel BFS time: {parallel_time:.4f}s")
print(f"Optimized parallel BFS time: {optimized_time:.4f}s")
print(f"Nodes reached: {len(distances_parallel)}")
print(f"Results match: {distances_parallel == distances_optimized}")
```

Answer: `future.result()` - Get the set of next-level nodes from each chunk processing task. Parallel BFS can achieve speedup by processing nodes at the same level concurrently, though synchronization overhead can limit scalability for graphs with small level sizes.

#@@@@@@@@@@

178. What is the purpose of the Lovász theta function in graph theory?

Answer: Lovász theta function ϑ(G) provides bounds for chromatic number and independence number of graph G. It satisfies α(G) ≤ ϑ(G) ≤ χ(Ḡ) where α is independence number and χ(Ḡ) is chromatic number of complement. Computable in polynomial time using semidefinite programming, useful for approximation algorithms.

#@@@@@@@@@@

179. Which of the following are valid approaches for the vertex cover problem? (Multiple correct)
A) 2-approximation using maximal matching
B) Brute force enumeration O(2^n)
C) Integer linear programming
D) Greedy algorithm (not optimal)
E) Parameterized algorithms
F) Branch and bound

Answer: A, B, C, D, E, F - All are valid approaches. 2-approximation via maximal matching, brute force for small instances, ILP for exact solution, greedy for fast approximation, parameterized algorithms for small vertex covers, and branch-and-bound for exact solutions with pruning.

#@@@@@@@@@@

180. Find the issue in this parallel merge implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor

def parallel_merge_arrays(arrays, max_workers=4):
    """Merge multiple sorted arrays in parallel"""
    if not arrays:
        return []

    if len(arrays) == 1:
        return arrays[0]

    # Merge arrays in pairs
    while len(arrays) > 1:
        next_round = []

        # Pair up arrays for parallel merging
        pairs = []
        for i in range(0, len(arrays), 2):
            if i + 1 < len(arrays):
                pairs.append((arrays[i], arrays[i + 1]))
            else:
                pairs.append((arrays[i], []))  # Odd array out

        if len(pairs) > 1:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit merge tasks
                futures = [
                    executor.submit(merge_two_arrays, pair[0], pair[1])
                    for pair in pairs
                ]

                # Collect results
                for future in futures:
                    merged = future.result()
                    if merged:  # Don't add empty arrays
                        next_round.append(merged)
        else:
            # Only one pair, merge sequentially
            merged = merge_two_arrays(pairs[0][0], pairs[0][1])
            if merged:
                next_round.append(merged)

        arrays = next_round

    return arrays[0] if arrays else []

def merge_two_arrays(arr1, arr2):
    """Merge two sorted arrays"""
    if not arr1:
        return arr2
    if not arr2:
        return arr1

    result = []
    i = j = 0

    while i < len(arr1) and j < len(arr2):
        if arr1[i] <= arr2[j]:
            result.append(arr1[i])
            i += 1
        else:
            result.append(arr2[j])
            j += 1

    # Add remaining elements
    result.extend(arr1[i:])
    result.extend(arr2[j:])

    return result

def k_way_merge_parallel(arrays, max_workers=4):
    """Alternative k-way merge using priority queue approach"""
    import heapq
    from concurrent.futures import ThreadPoolExecutor

    if not arrays:
        return []

    # Filter out empty arrays
    non_empty_arrays = [arr for arr in arrays if arr]

    if not non_empty_arrays:
        return []

    if len(non_empty_arrays) == 1:
        return non_empty_arrays[0]

    # Use heap for k-way merge
    heap = []
    array_indices = [0] * len(non_empty_arrays)

    # Initialize heap with first element from each array
    for i, arr in enumerate(non_empty_arrays):
        if arr:
            heapq.heappush(heap, (arr[0], i, 0))

    result = []

    while heap:
        value, array_idx, element_idx = heapq.heappop(heap)
        result.append(value)

        # Add next element from same array
        if element_idx + 1 < len(non_empty_arrays[array_idx]):
            next_value = non_empty_arrays[array_idx][element_idx + 1]
            heapq.heappush(heap, (next_value, array_idx, element_idx + 1))

    return result

# Test
import random

# Create test arrays
arrays = []
for i in range(8):
    size = random.randint(100, 1000)
    arr = sorted([random.randint(1, 10000) for _ in range(size)])
    arrays.append(arr)

# Test parallel merge
result1 = parallel_merge_arrays([arr.copy() for arr in arrays])

# Test k-way merge
result2 = k_way_merge_parallel([arr.copy() for arr in arrays])

# Test correctness
all_elements = []
for arr in arrays:
    all_elements.extend(arr)
expected = sorted(all_elements)

print(f"Parallel merge correct: {result1 == expected}")
print(f"K-way merge correct: {result2 == expected}")
print(f"Result length: {len(result1)}")
```

Answer: No explicit bug in the implementation. The parallel merge correctly handles pairing arrays, merging in parallel, and dealing with odd numbers of arrays. The k-way merge provides an alternative approach using a priority queue. Both implementations correctly merge multiple sorted arrays.

#@@@@@@@@@@

181. How do you implement the Strassen's algorithm for matrix multiplication?

Answer: Strassen's algorithm reduces matrix multiplication from O(n³) to O(n^2.807) using divide-and-conquer with 7 recursive multiplications instead of 8. Split matrices into quadrants, compute 7 products using specific linear combinations, then combine results. Practical for large matrices due to reduced asymptotic complexity.

#@@@@@@@@@@

182. Which of the following are applications of the minimum cut problem? (Multiple correct)
A) Image segmentation
B) Network reliability
C) VLSI design
D) Social network analysis
E) Clustering algorithms
F) Maximum flow computation

Answer: A, B, C, D, E, F - All are min cut applications: image segmentation (foreground/background), network reliability (bottleneck identification), VLSI design (circuit partitioning), social networks (community detection), clustering (graph partitioning), and max flow (max-flow min-cut theorem).

#@@@@@@@@@@

183. Complete this parallel dynamic programming solution:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
import numpy as np

def parallel_lcs(str1, str2, max_workers=4):
    """Parallel Longest Common Subsequence using diagonal computation"""
    m, n = len(str1), len(str2)

    # Initialize DP table
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    # Compute DP table diagonal by diagonal
    for diagonal in range(1, m + n + 1):
        # Collect cells in current diagonal
        diagonal_cells = []

        for i in range(1, m + 1):
            j = diagonal - i + 1
            if 1 <= j <= n:
                diagonal_cells.append((i, j))

        # Process diagonal cells in parallel
        if len(diagonal_cells) > max_workers:
            chunk_size = len(diagonal_cells) // max_workers
            chunks = [diagonal_cells[i:i + chunk_size] for i in range(0, len(diagonal_cells), chunk_size)]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(compute_lcs_chunk, chunk, str1, str2, dp)
                    for chunk in chunks
                ]

                # Wait for all chunks to complete
                for future in futures:
                    _______________
        else:
            # Sequential processing for small diagonals
            for i, j in diagonal_cells:
                if str1[i-1] == str2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])

    return dp[m][n]

def compute_lcs_chunk(cells, str1, str2, dp):
    """Compute LCS values for a chunk of diagonal cells"""
    for i, j in cells:
        if str1[i-1] == str2[j-1]:
            dp[i][j] = dp[i-1][j-1] + 1
        else:
            dp[i][j] = max(dp[i-1][j], dp[i][j-1])

def parallel_matrix_chain_multiplication(matrices, max_workers=4):
    """Parallel matrix chain multiplication using interval DP"""
    n = len(matrices)
    if n <= 1:
        return 0

    # dp[i][j] = minimum cost to multiply matrices from i to j
    dp = [[0] * n for _ in range(n)]

    # Process by chain length
    for length in range(2, n + 1):  # length of chain
        # Collect intervals of current length
        intervals = []
        for i in range(n - length + 1):
            j = i + length - 1
            intervals.append((i, j))

        # Process intervals in parallel
        if len(intervals) > max_workers:
            chunk_size = len(intervals) // max_workers
            chunks = [intervals[i:i + chunk_size] for i in range(0, len(intervals), chunk_size)]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(compute_mcm_chunk, chunk, matrices, dp)
                    for chunk in chunks
                ]

                for future in futures:
                    future.result()
        else:
            # Sequential processing
            for i, j in intervals:
                dp[i][j] = float('inf')
                for k in range(i, j):
                    cost = dp[i][k] + dp[k+1][j] + matrices[i] * matrices[k+1] * matrices[j+1]
                    dp[i][j] = min(dp[i][j], cost)

    return dp[0][n-1]

def compute_mcm_chunk(intervals, matrices, dp):
    """Compute matrix chain multiplication for chunk of intervals"""
    for i, j in intervals:
        dp[i][j] = float('inf')
        for k in range(i, j):
            cost = dp[i][k] + dp[k+1][j] + matrices[i] * matrices[k+1] * matrices[j+1]
            dp[i][j] = min(dp[i][j], cost)

# Test
import time

# Test parallel LCS
str1 = "ABCDGH" * 100  # Make strings longer for meaningful parallelization
str2 = "AEDFHR" * 100

start_time = time.time()
lcs_length = parallel_lcs(str1, str2)
parallel_time = time.time() - start_time

print(f"LCS length: {lcs_length}")
print(f"Parallel LCS time: {parallel_time:.4f}s")

# Test parallel matrix chain multiplication
matrices = [10, 20, 30, 40, 30, 20, 10]  # Dimensions of matrices

start_time = time.time()
min_cost = parallel_matrix_chain_multiplication(matrices)
mcm_time = time.time() - start_time

print(f"Minimum multiplication cost: {min_cost}")
print(f"Parallel MCM time: {mcm_time:.4f}s")
```

Answer: `future.result()` - Wait for each chunk computation to complete before proceeding to the next diagonal. Parallel dynamic programming can achieve speedup by processing independent subproblems concurrently, though synchronization requirements limit the parallelization opportunities.

#@@@@@@@@@@

184. What is the difference between approximation algorithms and heuristics?

Answer: Approximation algorithms provide provable performance guarantees (approximation ratio) compared to optimal solution, while heuristics may perform well in practice but lack theoretical guarantees. Approximation algorithms have worst-case analysis; heuristics rely on empirical evaluation. Both are used for NP-hard problems where exact solutions are impractical.

#@@@@@@@@@@

185. Which of the following are characteristics of cache-aware algorithms? (Multiple correct)
A) Designed for specific cache parameters
B) Minimize cache misses
C) Use blocking/tiling techniques
D) Consider memory hierarchy
E) Portable across different architectures
F) Always faster than cache-oblivious algorithms

Answer: A, B, C, D - Cache-aware algorithms are designed for specific cache parameters, minimize cache misses, use blocking techniques, and consider memory hierarchy. They're not portable across architectures (unlike cache-oblivious) and aren't always faster due to the overhead of cache-specific optimizations.

#@@@@@@@@@@

186. Predict the output of this string matching algorithm:
```python
def boyer_moore_search(text, pattern):
    """Boyer-Moore string matching with bad character rule"""
    def build_bad_char_table(pattern):
        table = {}
        for i in range(len(pattern)):
            table[pattern[i]] = i
        return table

    bad_char = build_bad_char_table(pattern)
    matches = []

    i = 0  # Index for text
    while i <= len(text) - len(pattern):
        j = len(pattern) - 1  # Start from end of pattern

        # Match pattern from right to left
        while j >= 0 and pattern[j] == text[i + j]:
            j -= 1

        if j < 0:
            # Pattern found
            matches.append(i)
            # Move to next possible position
            i += 1
        else:
            # Mismatch occurred, use bad character rule
            bad_char_shift = j - bad_char.get(text[i + j], -1)
            i += max(1, bad_char_shift)

    return matches

# Test
text = "ABAAABCDABABCABCABCABC"
pattern = "ABCAB"

matches = boyer_moore_search(text, pattern)
print(f"Pattern '{pattern}' found at positions: {matches}")

# Verify matches
for pos in matches:
    print(f"Position {pos}: '{text[pos:pos+len(pattern)]}'")
```

Answer: Pattern 'ABCAB' found at positions: [10, 16]
Position 10: 'ABCAB'
Position 16: 'ABCAB'

Boyer-Moore algorithm finds the pattern at positions 10 and 16 in the text. The bad character rule helps skip characters efficiently when mismatches occur, making it faster than naive string matching for many inputs.

#@@@@@@@@@@

187. How do you implement the Edmonds' blossom algorithm for maximum matching?

Answer: Edmonds' algorithm finds maximum matching in general graphs by handling odd cycles (blossoms). When augmenting path search encounters odd cycle, contract it into single vertex (blossom). Continue search on contracted graph. When augmenting path found, expand blossoms and lift path to original graph. Time: O(V³).

#@@@@@@@@@@

188. Which of the following are valid approaches for the knapsack problem variants? (Multiple correct)
A) 0/1 knapsack: Dynamic programming O(nW)
B) Fractional knapsack: Greedy algorithm O(n log n)
C) Unbounded knapsack: DP with unlimited items
D) Multiple knapsack: Generalization with multiple bags
E) Bounded knapsack: DP with item quantity limits
F) Subset sum: Special case with values equal weights

Answer: A, B, C, D, E, F - All are valid knapsack variants with appropriate solution approaches. 0/1 uses DP, fractional uses greedy by value/weight ratio, unbounded allows unlimited items, multiple has several knapsacks, bounded limits item quantities, subset sum is special case where value equals weight.

#@@@@@@@@@@

189. Complete this parallel graph coloring algorithm:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
import random

def parallel_graph_coloring(graph, max_workers=4):
    """Parallel graph coloring using largest-first heuristic"""
    n = len(graph)
    colors = [-1] * n

    # Calculate degrees
    degrees = [(len(graph[i]), i) for i in range(n)]
    degrees.sort(reverse=True)  # Largest degree first

    # Color vertices in parallel batches
    colored = set()
    color_counter = 0

    while len(colored) < n:
        # Find independent set of uncolored vertices
        independent_set = find_independent_set(graph, colored, degrees, max_workers)

        if not independent_set:
            break

        # Color all vertices in independent set with same color
        for vertex in independent_set:
            colors[vertex] = color_counter
            colored.add(vertex)

        color_counter += 1

    return colors

def find_independent_set(graph, colored, degrees, max_workers):
    """Find maximal independent set of uncolored vertices"""
    independent_set = []
    candidates = [v for _, v in degrees if v not in colored]

    if not candidates:
        return independent_set

    # Use parallel approach for large candidate sets
    if len(candidates) > max_workers * 10:
        chunk_size = len(candidates) // max_workers
        chunks = [candidates[i:i + chunk_size] for i in range(0, len(candidates), chunk_size)]

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(find_independent_chunk, chunk, graph, colored, independent_set)
                for chunk in chunks
            ]

            # Collect results from all chunks
            for future in futures:
                chunk_independent = _______________
                # Merge chunk results (ensuring independence)
                for vertex in chunk_independent:
                    if vertex not in independent_set:
                        # Check if vertex is independent of already selected vertices
                        is_independent = True
                        for selected in independent_set:
                            if selected in graph[vertex]:
                                is_independent = False
                                break

                        if is_independent:
                            independent_set.append(vertex)
    else:
        # Sequential approach for small candidate sets
        for vertex in candidates:
            # Check if vertex is independent of already selected vertices
            is_independent = True
            for selected in independent_set:
                if selected in graph[vertex]:
                    is_independent = False
                    break

            if is_independent:
                independent_set.append(vertex)

    return independent_set

def find_independent_chunk(chunk, graph, colored, current_independent):
    """Find independent vertices in a chunk"""
    chunk_independent = []

    for vertex in chunk:
        # Check if vertex is independent of colored vertices and current selection
        is_independent = True

        # Check against colored vertices
        for neighbor in graph[vertex]:
            if neighbor in colored:
                continue  # Colored vertices are not a problem

            # Check against current independent set
            if neighbor in current_independent or neighbor in chunk_independent:
                is_independent = False
                break

        if is_independent:
            chunk_independent.append(vertex)

    return chunk_independent

def sequential_graph_coloring(graph):
    """Sequential graph coloring for comparison"""
    n = len(graph)
    colors = [-1] * n

    for vertex in range(n):
        # Find smallest available color
        used_colors = set()
        for neighbor in graph[vertex]:
            if colors[neighbor] != -1:
                used_colors.add(colors[neighbor])

        color = 0
        while color in used_colors:
            color += 1

        colors[vertex] = color

    return colors

def verify_coloring(graph, colors):
    """Verify that coloring is valid"""
    for vertex in range(len(graph)):
        for neighbor in graph[vertex]:
            if colors[vertex] == colors[neighbor]:
                return False
    return True

# Test
def create_random_graph(n, edge_probability=0.3):
    """Create random graph for testing"""
    graph = [[] for _ in range(n)]

    for i in range(n):
        for j in range(i + 1, n):
            if random.random() < edge_probability:
                graph[i].append(j)
                graph[j].append(i)

    return graph

# Performance test
n = 1000
graph = create_random_graph(n)

import time

start_time = time.time()
parallel_colors = parallel_graph_coloring(graph)
parallel_time = time.time() - start_time

start_time = time.time()
sequential_colors = sequential_graph_coloring(graph)
sequential_time = time.time() - start_time

print(f"Parallel coloring time: {parallel_time:.4f}s")
print(f"Sequential coloring time: {sequential_time:.4f}s")
print(f"Parallel colors used: {max(parallel_colors) + 1}")
print(f"Sequential colors used: {max(sequential_colors) + 1}")
print(f"Parallel coloring valid: {verify_coloring(graph, parallel_colors)}")
print(f"Sequential coloring valid: {verify_coloring(graph, sequential_colors)}")
```

Answer: `future.result()` - Get the independent vertices found by each chunk processing task. Parallel graph coloring can achieve speedup by finding independent sets concurrently, though the sequential nature of color assignment limits the parallelization benefits.

#@@@@@@@@@@

190. What is the purpose of the Karger's algorithm for minimum cut?

Answer: Karger's algorithm finds minimum cut in undirected graph using randomized edge contraction. Repeatedly contract random edges until only 2 vertices remain. The remaining edges form a cut. Success probability is O(1/n²), so repeat O(n² log n) times for high success probability. Simple randomized algorithm with O(n⁴ log n) time.

#@@@@@@@@@@

191. Which of the following are applications of linear programming? (Multiple correct)
A) Network flow problems
B) Resource allocation
C) Portfolio optimization
D) Game theory (mixed strategies)
E) Approximation algorithms
F) Machine learning (SVM)

Answer: A, B, C, D, E, F - All are LP applications: network flow (max flow as LP), resource allocation (optimization with constraints), portfolio optimization (risk/return trade-offs), game theory (finding mixed Nash equilibria), approximation algorithms (LP relaxation), machine learning (SVM optimization).

#@@@@@@@@@@

192. Find the bug in this parallel sorting network implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor

def parallel_bitonic_sort(arr, max_workers=4):
    """Parallel bitonic sort implementation"""
    n = len(arr)

    # Ensure array length is power of 2
    if n & (n - 1) != 0:
        # Pad array to next power of 2
        next_power = 1
        while next_power < n:
            next_power <<= 1
        arr.extend([float('inf')] * (next_power - n))
        n = next_power

    # Bitonic sort
    k = 2
    while k <= n:
        j = k // 2
        while j > 0:
            # Parallel comparison and swap
            if j >= max_workers:
                parallel_compare_swap(arr, j, k, max_workers)
            else:
                sequential_compare_swap(arr, j, k)
            j //= 2
        k *= 2

    # Remove padding
    while arr and arr[-1] == float('inf'):
        arr.pop()

    return arr

def parallel_compare_swap(arr, j, k, max_workers):
    """Parallel compare and swap operations"""
    n = len(arr)
    operations = []

    # Collect all compare-swap operations
    for i in range(n):
        l = i ^ j  # XOR to find comparison partner
        if l > i:  # Avoid duplicate operations
            # Determine direction based on bitonic sequence structure
            direction = ((i & k) == 0)
            operations.append((i, l, direction))

    # Execute operations in parallel
    chunk_size = max(1, len(operations) // max_workers)
    chunks = [operations[i:i + chunk_size] for i in range(0, len(operations), chunk_size)]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(execute_swap_chunk, chunk, arr)
            for chunk in chunks
        ]

        # Wait for all operations to complete
        for future in futures:
            future.result()

def execute_swap_chunk(operations, arr):
    """Execute a chunk of compare-swap operations"""
    for i, l, direction in operations:
        if (arr[i] > arr[l]) == direction:
            arr[i], arr[l] = arr[l], arr[i]

def sequential_compare_swap(arr, j, k):
    """Sequential compare and swap for small j values"""
    n = len(arr)
    for i in range(n):
        l = i ^ j
        if l > i:
            direction = ((i & k) == 0)
            if (arr[i] > arr[l]) == direction:
                arr[i], arr[l] = arr[l], arr[i]

def verify_sorted(arr):
    """Verify that array is sorted"""
    for i in range(1, len(arr)):
        if arr[i] < arr[i-1]:
            return False
    return True

# Test
import random
import time

# Test with random array
test_sizes = [64, 128, 256]

for size in test_sizes:
    arr = [random.randint(1, 1000) for _ in range(size)]

    # Test parallel bitonic sort
    start_time = time.time()
    sorted_arr = parallel_bitonic_sort(arr.copy())
    parallel_time = time.time() - start_time

    # Test built-in sort
    start_time = time.time()
    expected = sorted(arr.copy())
    builtin_time = time.time() - start_time

    print(f"Size {size}:")
    print(f"  Parallel bitonic sort time: {parallel_time:.4f}s")
    print(f"  Built-in sort time: {builtin_time:.4f}s")
    print(f"  Correctly sorted: {sorted_arr == expected}")
    print(f"  Verification: {verify_sorted(sorted_arr)}")
    print()
```

Answer: No explicit bug in the implementation. The parallel bitonic sort correctly implements the bitonic sorting network with parallel compare-swap operations. The algorithm properly handles padding to power-of-2 size, executes comparison operations in parallel, and maintains the bitonic sorting network structure. The implementation correctly sorts the array using the bitonic merge pattern.

#@@@@@@@@@@

193. How do you implement the Coppersmith-Winograd algorithm for matrix multiplication?

Answer: Coppersmith-Winograd algorithm achieves O(n^2.376) matrix multiplication using advanced tensor techniques and trilinear forms. It's highly complex, involving recursive decomposition with carefully chosen basis transformations. While theoretically faster than Strassen's O(n^2.807), the large constants make it impractical for realistic matrix sizes.

#@@@@@@@@@@

194. Which of the following are characteristics of external memory algorithms? (Multiple correct)
A) Designed for data larger than RAM
B) Minimize disk I/O operations
C) Use cache-oblivious techniques
D) Block-based data access
E) Different complexity model (I/O model)
F) Always faster than internal algorithms

Answer: A, B, C, D, E - External memory algorithms handle data larger than RAM, minimize I/O operations, may use cache-oblivious techniques, access data in blocks, and use I/O complexity model counting disk accesses. They're not always faster due to I/O overhead for small datasets that fit in memory.

#@@@@@@@@@@

195. Complete this parallel topological sort implementation:
```python
import threading
from concurrent.futures import ThreadPoolExecutor
from collections import deque

def parallel_topological_sort(graph, max_workers=4):
    """Parallel topological sort using Kahn's algorithm"""
    n = len(graph)
    in_degree = [0] * n

    # Calculate in-degrees
    for u in range(n):
        for v in graph[u]:
            in_degree[v] += 1

    # Initialize queue with vertices having 0 in-degree
    queue = deque()
    for i in range(n):
        if in_degree[i] == 0:
            queue.append(i)

    result = []
    processed = 0

    while queue:
        # Process current level in parallel
        current_level = list(queue)
        queue.clear()

        # Add current level to result
        result.extend(current_level)
        processed += len(current_level)

        if current_level:
            # Process neighbors in parallel
            if len(current_level) > max_workers:
                chunk_size = len(current_level) // max_workers
                chunks = [current_level[i:i + chunk_size] for i in range(0, len(current_level), chunk_size)]

                # Thread-safe collections for next level
                next_level_queue = deque()
                queue_lock = threading.Lock()
                in_degree_lock = threading.Lock()

                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [
                        executor.submit(
                            process_topo_chunk,
                            chunk,
                            graph,
                            in_degree,
                            next_level_queue,
                            queue_lock,
                            in_degree_lock
                        )
                        for chunk in chunks
                    ]

                    # Wait for all chunks to complete
                    for future in futures:
                        _______________

                # Add next level vertices to queue
                queue.extend(next_level_queue)
            else:
                # Sequential processing for small levels
                for vertex in current_level:
                    for neighbor in graph[vertex]:
                        in_degree[neighbor] -= 1
                        if in_degree[neighbor] == 0:
                            queue.append(neighbor)

    # Check for cycles
    if processed != n:
        return None  # Cycle detected

    return result

def process_topo_chunk(chunk, graph, in_degree, next_level_queue, queue_lock, in_degree_lock):
    """Process a chunk of vertices in topological sort"""
    local_next_level = []

    for vertex in chunk:
        for neighbor in graph[vertex]:
            with in_degree_lock:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    local_next_level.append(neighbor)

    # Add to shared queue
    if local_next_level:
        with queue_lock:
            next_level_queue.extend(local_next_level)

def parallel_dfs_topological_sort(graph, max_workers=4):
    """Alternative parallel topological sort using DFS"""
    n = len(graph)
    visited = [False] * n
    rec_stack = [False] * n
    result = []
    result_lock = threading.Lock()

    def dfs(vertex):
        visited[vertex] = True
        rec_stack[vertex] = True

        for neighbor in graph[vertex]:
            if not visited[neighbor]:
                if not dfs(neighbor):
                    return False  # Cycle detected
            elif rec_stack[neighbor]:
                return False  # Back edge found (cycle)

        rec_stack[vertex] = False
        with result_lock:
            result.append(vertex)
        return True

    # Process unvisited vertices in parallel
    unvisited = list(range(n))

    while unvisited:
        # Find vertices with no incoming edges from unvisited vertices
        candidates = []
        for vertex in unvisited:
            has_incoming = False
            for u in unvisited:
                if vertex in graph[u]:
                    has_incoming = True
                    break
            if not has_incoming:
                candidates.append(vertex)

        if not candidates:
            # All remaining vertices are in cycles
            return None

        # Process candidates in parallel
        if len(candidates) > max_workers:
            chunk_size = len(candidates) // max_workers
            chunks = [candidates[i:i + chunk_size] for i in range(0, len(candidates), chunk_size)]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(dfs, chunk[0])  # Process one vertex per chunk
                    for chunk in chunks if chunk
                ]

                # Check for cycles
                for future in futures:
                    if not future.result():
                        return None
        else:
            # Sequential processing
            for vertex in candidates:
                if not dfs(vertex):
                    return None

        # Remove processed vertices
        unvisited = [v for v in unvisited if not visited[v]]

    return result[::-1]  # Reverse to get correct topological order

# Test
def create_dag(n, edge_probability=0.3):
    """Create a random DAG for testing"""
    import random
    graph = [[] for _ in range(n)]

    for i in range(n):
        for j in range(i + 1, n):  # Only forward edges to ensure DAG
            if random.random() < edge_probability:
                graph[i].append(j)

    return graph

def verify_topological_order(graph, order):
    """Verify that the order is a valid topological sort"""
    if order is None:
        return False

    position = {vertex: i for i, vertex in enumerate(order)}

    for u in range(len(graph)):
        for v in graph[u]:
            if position[u] >= position[v]:
                return False

    return True

# Performance test
n = 1000
graph = create_dag(n)

import time

start_time = time.time()
topo_order1 = parallel_topological_sort(graph)
parallel_time1 = time.time() - start_time

start_time = time.time()
topo_order2 = parallel_dfs_topological_sort(graph)
parallel_time2 = time.time() - start_time

print(f"Parallel Kahn's algorithm time: {parallel_time1:.4f}s")
print(f"Parallel DFS algorithm time: {parallel_time2:.4f}s")
print(f"Kahn's result valid: {verify_topological_order(graph, topo_order1)}")
print(f"DFS result valid: {verify_topological_order(graph, topo_order2)}")
print(f"Both found valid order: {topo_order1 is not None and topo_order2 is not None}")
```

Answer: `future.result()` - Wait for each chunk processing task to complete before proceeding to the next level. Parallel topological sort can achieve speedup by processing vertices at the same level concurrently, though the sequential dependencies limit the parallelization benefits.

#@@@@@@@@@@

196. What is the difference between competitive analysis and worst-case analysis?

Answer: Competitive analysis compares online algorithm performance to optimal offline algorithm on same input sequence, measuring competitive ratio. Worst-case analysis considers maximum time/space over all possible inputs for single algorithm. Competitive analysis is specific to online algorithms; worst-case analysis applies to any algorithm.

#@@@@@@@@@@

197. Which of the following are valid approaches for the facility location problem? (Multiple correct)
A) Greedy algorithm with approximation ratio
B) Linear programming relaxation
C) Local search heuristics
D) Primal-dual algorithms
E) Clustering-based approaches
F) Dynamic programming

Answer: A, B, C, D, E - All except DP are valid approaches. Greedy gives constant approximation, LP relaxation provides bounds and rounding schemes, local search finds good solutions, primal-dual gives approximation algorithms, clustering naturally relates to facility location. DP doesn't directly apply due to problem structure.

#@@@@@@@@@@

198. Predict the output of this computational geometry algorithm:
```python
def closest_pair_divide_conquer(points):
    """Find closest pair of points using divide and conquer"""
    def distance(p1, p2):
        return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5

    def closest_pair_rec(px, py):
        n = len(px)

        # Base case: brute force for small arrays
        if n <= 3:
            min_dist = float('inf')
            closest = None
            for i in range(n):
                for j in range(i + 1, n):
                    dist = distance(px[i], px[j])
                    if dist < min_dist:
                        min_dist = dist
                        closest = (px[i], px[j])
            return min_dist, closest

        # Divide
        mid = n // 2
        midpoint = px[mid]

        pyl = [point for point in py if point[0] <= midpoint[0]]
        pyr = [point for point in py if point[0] > midpoint[0]]

        # Conquer
        dl, pair_l = closest_pair_rec(px[:mid], pyl)
        dr, pair_r = closest_pair_rec(px[mid:], pyr)

        # Find minimum of the two halves
        if dl < dr:
            min_dist = dl
            closest_pair = pair_l
        else:
            min_dist = dr
            closest_pair = pair_r

        # Check points near the dividing line
        strip = [point for point in py if abs(point[0] - midpoint[0]) < min_dist]

        for i in range(len(strip)):
            j = i + 1
            while j < len(strip) and (strip[j][1] - strip[i][1]) < min_dist:
                dist = distance(strip[i], strip[j])
                if dist < min_dist:
                    min_dist = dist
                    closest_pair = (strip[i], strip[j])
                j += 1

        return min_dist, closest_pair

    # Sort points by x and y coordinates
    px = sorted(points, key=lambda p: p[0])
    py = sorted(points, key=lambda p: p[1])

    return closest_pair_rec(px, py)

# Test
points = [(2, 3), (12, 30), (40, 50), (5, 1), (12, 10), (3, 4)]
min_distance, pair = closest_pair_divide_conquer(points)

print(f"Closest pair: {pair}")
print(f"Distance: {min_distance:.4f}")

# Verify by checking all pairs
min_dist_brute = float('inf')
closest_brute = None

for i in range(len(points)):
    for j in range(i + 1, len(points)):
        dist = ((points[i][0] - points[j][0])**2 + (points[i][1] - points[j][1])**2)**0.5
        if dist < min_dist_brute:
            min_dist_brute = dist
            closest_brute = (points[i], points[j])

print(f"Brute force result: {closest_brute}")
print(f"Brute force distance: {min_dist_brute:.4f}")
print(f"Results match: {abs(min_distance - min_dist_brute) < 1e-10}")
```

Answer:
Closest pair: ((2, 3), (3, 4))
Distance: 1.4142
Brute force result: ((2, 3), (3, 4))
Brute force distance: 1.4142
Results match: True

The closest pair of points is (2,3) and (3,4) with distance √2 ≈ 1.4142. The divide-and-conquer algorithm correctly finds the closest pair in O(n log n) time by recursively dividing the point set and checking points near the dividing line.

#@@@@@@@@@@

199. How do you implement the Viterbi algorithm for Hidden Markov Models?

Answer: Viterbi algorithm finds most likely sequence of hidden states using dynamic programming. Create DP table where dp[t][s] = max probability of being in state s at time t. For each time step: dp[t][s] = max over previous states of (dp[t-1][prev] × transition[prev][s] × emission[s][observation[t]]). Backtrack to reconstruct optimal path.

#@@@@@@@@@@

200. Which of the following represent the current frontiers in algorithm research? (Multiple correct)
A) Quantum algorithms
B) Streaming algorithms for big data
C) Approximation algorithms for NP-hard problems
D) Parallel and distributed algorithms
E) Machine learning algorithm optimization
F) Fine-grained complexity theory

Answer: A, B, C, D, E, F - All represent active research frontiers: quantum algorithms (Shor's, Grover's extensions), streaming algorithms (sublinear space), approximation algorithms (better ratios, inapproximability), parallel algorithms (multicore, GPU), ML optimization (neural architecture search, federated learning), fine-grained complexity (conditional lower bounds, SETH). These areas drive modern algorithmic research and have significant practical impact.

#@@@@@@@@@@